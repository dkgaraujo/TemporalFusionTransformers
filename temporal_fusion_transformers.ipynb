{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Experimenting with TFT\"\n",
    "author: Douglas Araujo\n",
    "format: \n",
    "    html:\n",
    "        toc: true\n",
    "        toc-location: right\n",
    "        toc-depth: 4\n",
    "        number-sections: true\n",
    "        code-fold: true\n",
    "        code-tools: true\n",
    "        embed-resources: true\n",
    "bibliography: ref.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the temporal fusion transformers [@lim2021temporal] architecture, and ports it over to keras 3 while making some punctual improvements, including making the notation closer to the paper math.\n",
    "\n",
    "The original repository is: https://github.com/google-research/google-research/tree/master/tft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import keras_core as keras\n",
    "from keras_core import layers\n",
    "from fastcore import docments\n",
    "from nbdev.showdoc import show_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main characteristics of TFT that make it interesting for nowcasting or forecasting purposes are:\n",
    "\n",
    "- **multi-horizon forecasting**: the ability to output, at each point in time $t$, a sequence of forecasts for $t+h, h > 1$\n",
    "- **quantile prediction**: each forecast is accompanied by a quantile band that communicates the amount of uncertainty around a prediction\n",
    "- **flexible use of different types of inputs**: static inputs (akin to fixed effects), historical input and known future input (eg, important holidays, years that are known to have major sports events such as Olympic games, etc)\n",
    "- **interpretability**: the model learns to select variables from the space of all input variables to retain only those that are globally meaningful, to assign attention to different parts of the time series, and to identify events of significance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **time distributed**: \n",
    "  * applies same layer to each of the timesteps in the data\n",
    "    * in other words, a layer with the exact same weights\n",
    "  * indices:\n",
    "    * index 0: batch\n",
    "    * index 1: time\n",
    "    * indices 2...: data\n",
    "  * More info: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer\n",
    "\n",
    "> dedicated implementation to better control use of time distribution on vanilla linear layer\n",
    "\n",
    "$$\n",
    "\\mathbb{Y} = \\phi(\\mathbf{W} x + \\mathbf{b}),\n",
    "$$ {#eq-dense}\n",
    "\n",
    "where $x$ is the input to `linear_layer()(x)`, $\\mathbb{Y}$ is the output of `linear_layer()(x)`, $\\phi$ is an activation function (or no activation function is `activation` is `None`), $\\mathbf{W} \\in \\mathbf{R}^{(d_{\\text{size}} \\times d_{\\text{inputs}})}$ is a matrix of weights and $\\mathbf{b} \\in \\mathbf{R}^{d_{size}}$ is a vector of biases. Importantly, $\\mathbf{W}$ and $\\mathbf{b}$ are indexed with $_{\\omega}$ to denote weight-sharing when the layer is time-distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(size:int, # Output size\n",
    "                 activation:str|callable|None=None, # Activation function\n",
    "                 use_time_distributed:bool=False, # Apply the layer across all time steps?\n",
    "                 use_bias:bool=True # Include bias in the layer?\n",
    ")->layers.Dense: # Dense layer\n",
    "    \"Linear layer.\"\n",
    "\n",
    "    linear = layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "    if use_time_distributed:\n",
    "        linear = layers.TimeDistributed(linear)\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### linear_layer\n",
       "\n",
       ">      linear_layer (size:int, activation:Union[str,<built-\n",
       ">                    infunctioncallable>,NoneType]=None,\n",
       ">                    use_time_distributed:bool=False, use_bias:bool=True)\n",
       "\n",
       "Linear layer.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| size | int |  | Output size |\n",
       "| activation | str \\| callable \\| None | None | Activation function |\n",
       "| use_time_distributed | bool | False | Apply the layer across all time steps? |\n",
       "| use_bias | bool | True | Include bias in the layer? |\n",
       "| **Returns** | **layers.Dense** |  | **Dense layer** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### linear_layer\n",
       "\n",
       ">      linear_layer (size:int, activation:Union[str,<built-\n",
       ">                    infunctioncallable>,NoneType]=None,\n",
       ">                    use_time_distributed:bool=False, use_bias:bool=True)\n",
       "\n",
       "Linear layer.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| size | int |  | Output size |\n",
       "| activation | str \\| callable \\| None | None | Activation function |\n",
       "| use_time_distributed | bool | False | Apply the layer across all time steps? |\n",
       "| use_bias | bool | True | Include bias in the layer? |\n",
       "| **Returns** | **layers.Dense** |  | **Dense layer** |"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "show_doc(linear_layer, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 2\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "layer_size = 8\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.ones(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# dense layer\n",
    "linear_td_true = linear_layer(size=layer_size, use_time_distributed=True)\n",
    "linear_td_false = linear_layer(size=layer_size, use_time_distributed=False)\n",
    "\n",
    "# output dimensions: batches / timesteps / layer size\n",
    "assert linear_td_true(x).shape == [batch_size, n_timesteps, layer_size]\n",
    "assert linear_td_false(x).shape == [batch_size, n_timesteps, layer_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the time-distributed linear layer results in the same weights being applied to each time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 8), dtype=float32, numpy=\n",
       "array([[[-2.1470957 ,  1.0115887 , -0.6940057 ,  0.9228788 ,\n",
       "          0.31520468, -0.12253807, -0.6000036 ,  1.6961169 ],\n",
       "        [-2.1470957 ,  1.0115887 , -0.6940057 ,  0.9228788 ,\n",
       "          0.31520468, -0.12253807, -0.6000036 ,  1.6961169 ],\n",
       "        [-2.1470957 ,  1.0115887 , -0.6940057 ,  0.9228788 ,\n",
       "          0.31520468, -0.12253807, -0.6000036 ,  1.6961169 ],\n",
       "        [-2.1470957 ,  1.0115887 , -0.6940057 ,  0.9228788 ,\n",
       "          0.31520468, -0.12253807, -0.6000036 ,  1.6961169 ],\n",
       "        [-2.1470957 ,  1.0115887 , -0.6940057 ,  0.9228788 ,\n",
       "          0.31520468, -0.12253807, -0.6000036 ,  1.6961169 ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "x = np.ones((1, n_timesteps, n_features))\n",
    "timesteps_equal = []\n",
    "for i in range(n_timesteps-1):\n",
    "    timesteps_equal.append((np.array_equal(linear_td_true(x)[0,0,:], linear_td_true(x)[0,i+1,:])))\n",
    "\n",
    "assert np.all(timesteps_equal)\n",
    "\n",
    "linear_td_true(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip connection\n",
    "\n",
    "> Adds inputs to layer and then implements layer normalisation\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(a + b),\n",
    "$$ {#eq-skip}\n",
    "\n",
    "for $a$ and $b$ tensors of the same dimension and $\\text{LayerNorm}(\\cdot)$ being the layer normalisation (@ba2016layer), ie subtracting $\\mu^l$ and dividing by $\\sigma^l$ defined as:\n",
    "\n",
    "$$\n",
    "\\mu^l = \\frac{1}{H} \\sum_{i=1}^H n_i^l \\quad \\sigma^l = \\sqrt{\\frac{1}{H} \\sum_{i=1}^H (n_i^l - \\mu^l)^2},\n",
    "$$ {#eq-layernorm}\n",
    "\n",
    "with $H$ denoting the number of $n$ hidden units in a layer $l$.\n",
    "\n",
    "* Adding a layer's inputs to its outputs is also called \"skip connection\"\n",
    "* The layer is then normalised [@ba2016layer] to avoid having the numbers grow too big, which is detrimental for gradient transmission\n",
    "  * Layer normalisation uses the same computation both during training and inference times, and is particularly suitable for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_norm(\n",
    "    x_list # List of input tensors (of the same dimension) for skip connection\n",
    "    ):\n",
    "    \"Adds tensors with same dimensions and then normalises layer\"\n",
    "    tmp = layers.Add()(x_list)\n",
    "    return layers.LayerNormalization()(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### add_and_norm\n",
       "\n",
       ">      add_and_norm (x_list)\n",
       "\n",
       "Adds tensors with same dimensions and then normalises layer\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| x_list | List of input tensors (of the same dimension) for skip connection |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### add_and_norm\n",
       "\n",
       ">      add_and_norm (x_list)\n",
       "\n",
       "Adds tensors with same dimensions and then normalises layer\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| x_list | List of input tensors (of the same dimension) for skip connection |"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "show_doc(add_and_norm, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x1 = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "x2 = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# output dimensions: batches / timesteps / features\n",
    "x1x2 = add_and_norm(x_list=[x1, x2])\n",
    "assert x1.shape == x1x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean values of the normalised layer should be around 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of x1 units at each batch X time step:  [[ 0.07455922  0.04061129  0.05181068 -0.09088949 -0.07739758]\n",
      " [ 0.01492953 -0.16913738 -0.17806861  0.02542704 -0.02922696]\n",
      " [-0.04603776  0.09231065 -0.07678928  0.01013863  0.01639121]] \n",
      "\n",
      "mean of x2 units at each batch X time step:  [[ 0.00980258 -0.00192174  0.06792247 -0.05294029 -0.06777942]\n",
      " [ 0.00694195 -0.1447223  -0.0799674   0.11319608  0.07691094]\n",
      " [-0.04790915 -0.06469555 -0.03416057 -0.04981646  0.18596057]] \n",
      "\n",
      "mean of sum of x1 and x2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " array([[-2.3841858e-08, -1.1473894e-08,  9.5367430e-09,  2.3841857e-09,\n",
       "         -7.1525572e-09],\n",
       "        [-1.3113022e-08,  2.0861625e-08, -5.9604646e-09,  1.9073486e-08,\n",
       "          5.9604646e-09],\n",
       "        [-1.1920929e-08, -1.9073486e-08, -1.9073486e-08,  1.9073486e-08,\n",
       "          5.8114527e-09]], dtype=float32))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"mean of x1 units at each batch X time step: \", x1.mean(axis=-1), \"\\n\")\n",
    "print(\"mean of x2 units at each batch X time step: \", x2.mean(axis=-1), \"\\n\")\n",
    "print(\"mean of sum of x1 and x2:\"), x1x2.numpy().mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation (for the normalised output it should be around 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std of x1 units at each batch X time step:  [[1.02569605 0.98900181 0.94005451 0.89980367 0.96177263]\n",
      " [1.17489033 1.05453288 0.90757409 0.92312574 0.94491605]\n",
      " [0.99893797 0.96701721 0.97790858 0.9491741  1.09512876]] \n",
      "\n",
      "std of x2 units at each batch X time step:  [[0.8787866  0.95885331 0.9797253  0.97875105 1.04979644]\n",
      " [0.96219559 0.92205835 1.03307451 0.88680021 0.93463915]\n",
      " [1.05767775 1.06415902 0.98560554 1.0963727  0.92135964]] \n",
      "\n",
      "std of normalised sum of x1 and x2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " array([[0.9997369 , 0.99976975, 0.9997241 , 0.9997288 , 0.9997938 ],\n",
       "        [0.999778  , 0.9997712 , 0.99977314, 0.9996005 , 0.99971753],\n",
       "        [0.99974185, 0.99978566, 0.99975353, 0.99979323, 0.99974686]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"std of x1 units at each batch X time step: \", x1.std(axis=-1), \"\\n\")\n",
    "print(\"std of x2 units at each batch X time step: \", x2.std(axis=-1), \"\\n\")\n",
    "print(\"std of normalised sum of x1 and x2:\"), x1x2.numpy().std(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated linear unit (GLU)\n",
    "\n",
    "> Linear layer that learns how much to gate vs let pass through\n",
    "\n",
    "Using input $\\gamma \\in \\mathbb{R}^{d_{\\text{model}}}$ and the subscript $\\omega$ to index weights, \n",
    "\n",
    "$$\n",
    "\\text{GLU}_{\\omega}(\\gamma) = \\sigma(W_{4, \\omega} \\gamma + b_{4, \\omega}) \\odot (W_{5, \\omega} \\gamma + b_{5, \\omega}).\n",
    "$$ {#eq-GLU}\n",
    "\n",
    "* Introduced by @dauphin2017language\n",
    "* The intuition is to train two versions of @eq-dense in the same data, but one of them having a sigmoid activation (which outputs values between zero and one), then multiplying each hidden unit\n",
    "* The result could be zero or very close to zero through the Hadamard multipliciation, which in practice means that the network would not be affected by that data (ie, it would be gated out)\n",
    "  * The first term (with the sigmoid) is the gate that determines what percentage of the linear layer passes through\n",
    "* *\"GLUs reduce the vanishing gradient problem for deep architectures by providing a linear path for gradients while retaining non-linear capabilities\"*\n",
    "* *\"provide flexibility to suppress any parts of the architecture that are not required for a given dataset\"*\n",
    "* The GLU is part of @sec-GRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "\n",
    "def apply_gating_layer(\n",
    "    x, # Input tensors (batch first)\n",
    "    hidden_layer_size:int, # Dimension of the GLU\n",
    "    dropout_rate:float|None=None, # Dropout rate\n",
    "    use_time_distributed:bool=True, # Apply the GLU across all timesteps?\n",
    "    activation:str|callable=None # Activation function\n",
    "): # Tuple of (GLU output tensors, gated_layer)\n",
    "    \"Gated Linear Unit (GLU) layer\"\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    activation_layer = linear_layer(\n",
    "        size=hidden_layer_size,\n",
    "        activation=activation,\n",
    "        use_time_distributed=use_time_distributed\n",
    "    )(x)\n",
    "    \n",
    "    gate_layer = linear_layer(\n",
    "        size=hidden_layer_size,\n",
    "        activation='sigmoid',\n",
    "        use_time_distributed=use_time_distributed\n",
    "    )(x)\n",
    "\n",
    "    return layers.Multiply()([activation_layer, gate_layer]), gate_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### apply_gating_layer\n",
       "\n",
       ">      apply_gating_layer (x, hidden_layer_size:int,\n",
       ">                          dropout_rate:Optional[float]=None,\n",
       ">                          use_time_distributed:bool=True,\n",
       ">                          activation:Union[str,<built-\n",
       ">                          infunctioncallable>]=None)\n",
       "\n",
       "Gated Linear Unit (GLU) layer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Input tensors (batch first) |\n",
       "| hidden_layer_size | int |  | Dimension of the GLU |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GLU across all timesteps? |\n",
       "| activation | str \\| callable | None | Activation function |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### apply_gating_layer\n",
       "\n",
       ">      apply_gating_layer (x, hidden_layer_size:int,\n",
       ">                          dropout_rate:Optional[float]=None,\n",
       ">                          use_time_distributed:bool=True,\n",
       ">                          activation:Union[str,<built-\n",
       ">                          infunctioncallable>]=None)\n",
       "\n",
       "Gated Linear Unit (GLU) layer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Input tensors (batch first) |\n",
       "| hidden_layer_size | int |  | Dimension of the GLU |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GLU across all timesteps? |\n",
       "| activation | str \\| callable | None | Activation function |"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "show_doc(apply_gating_layer, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "hidden_layer_size = 16\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# output dimensions: batches / timesteps / hidden_layer_size\n",
    "assert apply_gating_layer(x=x, hidden_layer_size=hidden_layer_size)[0].shape == [batch_size, n_timesteps, hidden_layer_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated residual network (GRN) {#sec-GRN}\n",
    "\n",
    "$$\n",
    "\\text{GRN}_{\\omega}(a, c) = \\text{LayerNorm}(a + \\text{GLU}_{\\omega}(W_{1, \\omega} \\text{ELU}(W_{2, \\omega} a + b_{2, \\omega} + W_{3, \\omega} c) + b_{1, w}))\n",
    "$$ {#eq-GRN}\n",
    "\n",
    "* Breaking down $\\text{GRN}_{\\omega}(a, c)$:\n",
    "    * *1st step*: $\\eta_{2} = \\text{ELU}(W_{2, \\omega} a + b_{2, \\omega} + W_{3, \\omega} c)$ (where the additional context $c$ might be zero) as in @eq-dense but adapted for the added context if any and with $\\text{ELU}(\\cdot)$ as the activation function,\n",
    "    * *2nd step*: $\\eta_{1} = W_{1, \\omega} \\eta_{2} + b_{1, w}$ as in @eq-dense,\n",
    "    * *3rd step*: $\\text{LayerNorm}(a + \\text{GLU}_{\\omega}(\\eta_{1}))$ as in @eq-skip and @eq-GLU\n",
    "* $\\text{ELU}(\\cdot)$ is the Exponential Linear Unit activation function (@clevert2015fast)\n",
    "    * Unlike ReLUs, ELUs allow for negative values, which pushes unit activations closer to zero at a lower computation complexity, and producing more accurate results\n",
    "* The GRN is a key building block of the TFT\n",
    "    * Helps keep information only from relevant input variables\n",
    "    * Also keeps the model as simple as possible by only applying non-linearities when relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_residual_network(\n",
    "    x, # Network inputs\n",
    "    hidden_layer_size:int, # Dimension of the GRN\n",
    "    output_size:int|None=None, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "    dropout_rate:float|None=None, # Dropout rate\n",
    "    use_time_distributed:bool=True, # Apply the GRN across all timesteps?\n",
    "    additional_context=None, # Additional context vector to use if relevant\n",
    "    return_gate:bool=False #Whether to return GRN gate for diagnostic purposes\n",
    "):\n",
    "    \"Applies the gated residual network (GRN) as defined in the paper\"\n",
    "    \n",
    "    # Setup skip connection\n",
    "    if output_size is None:\n",
    "        output_size = hidden_layer_size\n",
    "        skip = x\n",
    "    else:\n",
    "        linear = keras.layers.Dense(output_size)\n",
    "        if use_time_distributed:\n",
    "            linear = keras.layers.TimeDistributed(linear)\n",
    "        skip = linear(x)\n",
    "\n",
    "    # 1st step: eta2\n",
    "    hidden = linear_layer(\n",
    "        size=hidden_layer_size, # W2\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=True # b2\n",
    "    )(x)\n",
    "\n",
    "    # \"For instances without a context vector, the GRN simply treates the context input as zero - ie, $c = 0$ in Eq. 4\"\n",
    "    if additional_context is not None: # if c is != 0...\n",
    "        hidden += linear_layer(\n",
    "            size=hidden_layer_size, # W3\n",
    "            activation=None,\n",
    "            use_time_distributed=use_time_distributed,\n",
    "            use_bias=False # no bias for additional context, since there already is bias from the \"main\" calculation of eta2\n",
    "        )(additional_context)\n",
    "\n",
    "    hidden = keras.layers.Activation('elu')(hidden)\n",
    "\n",
    "    # 2nd step: eta1\n",
    "    hidden = linear_layer(\n",
    "        size=hidden_layer_size, # W1\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=True # b1\n",
    "    )(hidden)\n",
    "\n",
    "    # 3rd step: concluding the GRN calculation\n",
    "    gating_layer, gate = apply_gating_layer(\n",
    "        x=hidden,\n",
    "        hidden_layer_size=output_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        activation=None\n",
    "    )\n",
    "\n",
    "    GRN = add_and_norm([skip, gating_layer])\n",
    "\n",
    "    if return_gate:\n",
    "        return GRN, gate\n",
    "    else:\n",
    "        return GRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### gated_residual_network\n",
       "\n",
       ">      gated_residual_network (x, hidden_layer_size:int,\n",
       ">                              output_size:Optional[int]=None,\n",
       ">                              dropout_rate:Optional[float]=None,\n",
       ">                              use_time_distributed:bool=True,\n",
       ">                              additional_context=None, return_gate:bool=False)\n",
       "\n",
       "Applies the gated residual network (GRN) as defined in the paper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Network inputs |\n",
       "| hidden_layer_size | int |  | Dimension of the GRN |\n",
       "| output_size | int \\| None | None | Size of output layer (if None, same as `hidden_layer_size`) |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GRN across all timesteps? |\n",
       "| additional_context | NoneType | None | Additional context vector to use if relevant |\n",
       "| return_gate | bool | False | Whether to return GRN gate for diagnostic purposes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### gated_residual_network\n",
       "\n",
       ">      gated_residual_network (x, hidden_layer_size:int,\n",
       ">                              output_size:Optional[int]=None,\n",
       ">                              dropout_rate:Optional[float]=None,\n",
       ">                              use_time_distributed:bool=True,\n",
       ">                              additional_context=None, return_gate:bool=False)\n",
       "\n",
       "Applies the gated residual network (GRN) as defined in the paper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Network inputs |\n",
       "| hidden_layer_size | int |  | Dimension of the GRN |\n",
       "| output_size | int \\| None | None | Size of output layer (if None, same as `hidden_layer_size`) |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GRN across all timesteps? |\n",
       "| additional_context | NoneType | None | Additional context vector to use if relevant |\n",
       "| return_gate | bool | False | Whether to return GRN gate for diagnostic purposes |"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "show_doc(gated_residual_network, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "hidden_layer_size = 16\n",
    "output_size = 17\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "grn = gated_residual_network(\n",
    "    x=x,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=0,\n",
    "    use_time_distributed=True,\n",
    "    additional_context=None,\n",
    "    return_gate=False\n",
    ")\n",
    "\n",
    "# output dimensions: batches / timesteps / hidden_layer_size\n",
    "assert grn.shape == [batch_size, n_timesteps, output_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention components\n",
    "\n",
    "* Attention mechanisms use relationships between keys $K \\in \\mathbf{R}^{N \\times d_{attention}}$ and queries $Q \\in \\mathbf{R}^{N \\times d_{attention}}$ to scale a vector of values $V \\in \\mathbf{R}^{N \\times d_V}$: $\\text{Attention}(Q, K, V) = A(Q, K) V$\n",
    "    * $N$ is the number of timesteps going into the attention layer (the number of lags plus the number of periods to be forecasted)\n",
    "    * $A(\\cdot)$ is a normalisation function\n",
    "        * After @vaswani2017attention, the canonical choice for $A(\\cdot)$ is the scaled dot-product: $A(Q, K) = \\text{Softmax}(\\frac{Q K^{T}}{\\sqrt{d_{attention}}} )$\n",
    "    \n",
    "* The TFT uses a modified attention head to enhance the explainability of the model\n",
    "* Specifically, the transformer block (multi-head attention) is modified to:\n",
    "    * share values in each head, and\n",
    "    * employ additive aggregation of all heads\n",
    "* More formally, compare the interpretable multi-head attention (used in this paper) with the canonical multi-head attention:\n",
    "    * $\\text{InterpretableMultiHead}(Q, K, V) = \\tilde{H} W_{H}$, with:\n",
    "        * $\\begin{aligned}\\tilde{H} &= \\tilde{A}(Q, K) V W_V \\\\\n",
    "        &= \\{\\frac{1}{m_H} \\sum^{m_{H}}_{h=1} A(Q W^{(h)}_Q, K W^{(h)}_K) \\} V W_V \\\\\n",
    "        &= \\frac{1}{m_H} \\sum^{m_{H}}_{h=1} \\text{Attention}(Q W^{(h)}_Q, K W^{(h)}_K, V W_V)\n",
    "        \\end{aligned}$\n",
    "    * $\\text{MultiHead}(Q, K, V) = [H_1, \\dots, H_{m_H}] W_H$, with:\n",
    "        * $H_h = \\text{Attention}(Q W^{(h)}_Q, K W^{(h)}_K, V W_V^{(h)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder mask for self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_mask(\n",
    "    self_attention_inputs # Inputs to the self-attention layer\n",
    "):\n",
    "    \"Determines shape of decoder mask\"\n",
    "    len_s = keras.ops.shape(self_attention_inputs)[1] # length of inputs\n",
    "    bs = keras.ops.shape(self_attention_inputs)[0] # batch shape\n",
    "    mask = keras.ops.cumsum(keras.ops.eye(len_s), 1) #keras.backend.cumsum(np.eye(len_s, bs))\n",
    "\n",
    "    ### warning: I had to manually implement some batch-wise shape here \n",
    "    ### because the new keras `eye` function does not have a batch_size arg.\n",
    "    ### inspired by: https://github.com/tensorflow/tensorflow/blob/v2.14.0/tensorflow/python/ops/linalg_ops_impl.py#L30\n",
    "    ### <hack>\n",
    "    mask = keras.ops.expand_dims(mask, axis=0)    \n",
    "    mask = keras.ops.tile(mask, (bs, 1, 1))\n",
    "    ### </hack>\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of the decoder mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = get_decoder_mask(grn)\n",
    "\n",
    "assert dec.shape == (batch_size, n_timesteps, n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it produces an upper-triangular matrix of ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_decoder_mask\n",
       "\n",
       ">      get_decoder_mask (self_attention_inputs)\n",
       "\n",
       "Determines shape of decoder mask\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| self_attention_inputs | Inputs to the self-attention layer |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_decoder_mask\n",
       "\n",
       ">      get_decoder_mask (self_attention_inputs)\n",
       "\n",
       "Determines shape of decoder mask\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| self_attention_inputs | Inputs to the self-attention layer |"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_decoder_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention layer\n",
    "\n",
    "* This is the same as Eq. (1) of @vaswani2017attention \n",
    "    * except that in this case the dimension of the value vector is the same $d_{\\text{model}}$ as for the query and key vectors\n",
    "* As discussed in the paper, additive attention outperforms dot product attention for larger $d_{\\text{model}}$ values, so the attention is scaled back to smaller values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention():\n",
    "    def __init__(\n",
    "        self,\n",
    "        training:bool=True, # Whether the layer is being trained or used in inference\n",
    "        attention_dropout:float=0.0 # Will be ignored if `training=False`\n",
    "    ):\n",
    "        self.training = training\n",
    "        self.dropout = keras.layers.Dropout(rate=attention_dropout)\n",
    "        self.activation = keras.layers.Activation('softmax')\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q, # Queries, tensor of shape (?, time, D_model)\n",
    "        k, # Keys, tensor of shape (?, time, D_model)\n",
    "        v, # Values, tensor of shape (?, time, D_model)\n",
    "        mask # Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)\n",
    "    ):\n",
    "        # returns Tuple (layer outputs, attention weights)\n",
    "        scale = keras.ops.sqrt(keras.ops.cast(keras.ops.shape(k)[-1], dtype='float32'))\n",
    "        attention = keras.ops.einsum(\"bij,bjk->bik\", q, keras.ops.transpose(k, axes=(0, 2, 1))) / scale\n",
    "        if mask is not None:\n",
    "            mmask = keras.layers.Lambda(lambda x: (-1e9) * (1. - keras.ops.cast(x, 'float32')))(mask)\n",
    "            attention = keras.layers.Add()([attention, mmask])\n",
    "        attention = self.activation(attention)\n",
    "        if self.training:\n",
    "            attention = self.dropout(attention)\n",
    "        output = keras.ops.einsum(\"btt,btd->bt\", attention, v)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how the `ScaledDotProductAttention` layer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 13\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x_btf = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# using the same vector for q, k and v just to simplify\n",
    "q=keras.ops.cast(x_btf, 'float32')\n",
    "k=keras.ops.cast(x_btf, 'float32')\n",
    "v=keras.ops.cast(x_btf, 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing without masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[-1.3595526 ,  2.3038948 , -0.9351378 , -2.1107376 , -0.10875011],\n",
       "        [ 0.74828297,  1.2987878 ,  2.7947266 ,  3.3696222 ,  0.02006484],\n",
       "        [-4.0625644 ,  7.9136066 ,  0.15763263,  0.387267  ,  0.5300054 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[8.67120624e-01, 7.71935284e-02, 1.09112160e-02, 3.08754463e-02,\n",
       "          1.38991298e-02],\n",
       "         [1.48252323e-01, 7.48296022e-01, 6.03066683e-02, 2.28216760e-02,\n",
       "          2.03233790e-02],\n",
       "         [4.45502140e-02, 1.28209814e-01, 7.00278938e-01, 7.39124417e-02,\n",
       "          5.30485511e-02],\n",
       "         [2.03909084e-01, 7.84784630e-02, 1.19554065e-01, 5.13916612e-01,\n",
       "          8.41417834e-02],\n",
       "         [5.25127314e-02, 3.99808772e-02, 4.90878299e-02, 4.81354706e-02,\n",
       "          8.10283065e-01]],\n",
       " \n",
       "        [[9.24033284e-01, 1.75150018e-02, 4.11768891e-02, 4.70066769e-03,\n",
       "          1.25742713e-02],\n",
       "         [9.32998657e-02, 6.94437802e-01, 1.02914162e-01, 7.62115791e-02,\n",
       "          3.31365839e-02],\n",
       "         [1.35585129e-01, 6.36154786e-02, 6.71839118e-01, 8.62094313e-02,\n",
       "          4.27508503e-02],\n",
       "         [1.26347877e-02, 3.84555049e-02, 7.03727603e-02, 8.37388158e-01,\n",
       "          4.11488079e-02],\n",
       "         [1.19896367e-01, 5.93143813e-02, 1.23796776e-01, 1.45972848e-01,\n",
       "          5.51019549e-01]],\n",
       " \n",
       "        [[9.73589659e-01, 1.44029455e-02, 9.85875167e-03, 9.39828868e-04,\n",
       "          1.20880269e-03],\n",
       "         [5.55578293e-03, 9.81782973e-01, 2.12343060e-03, 9.56264790e-03,\n",
       "          9.75145609e-04],\n",
       "         [2.00642303e-01, 1.12032667e-01, 3.68449330e-01, 2.26760492e-01,\n",
       "          9.21152383e-02],\n",
       "         [1.68646057e-03, 4.44847867e-02, 1.99937429e-02, 9.19841528e-01,\n",
       "          1.39935594e-02],\n",
       "         [4.58808010e-03, 9.59513243e-03, 1.71793364e-02, 2.95989700e-02,\n",
       "          9.39038575e-01]]], dtype=float32)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "output, attention = ScaledDotProductAttention()(q=q, k=k, v=v, mask=None)\n",
    "output, attention # both have shape (batch_size, n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and with masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[-1.3595526 ,  2.704903  , -1.130431  , -3.5293167 , -0.1342125 ],\n",
       "        [ 0.74828297,  1.4324336 ,  3.4899209 ,  3.8354926 ,  0.03641403],\n",
       "        [-4.0625644 ,  7.9578185 ,  0.22934216,  0.414706  ,  0.56441283]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[8.67120624e-01, 7.71935284e-02, 1.09112160e-02, 3.08754463e-02,\n",
       "          1.38991298e-02],\n",
       "         [0.00000000e+00, 8.78541887e-01, 7.08034411e-02, 2.67939400e-02,\n",
       "          2.38607973e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 8.46524537e-01, 8.93482491e-02,\n",
       "          6.41271621e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.59308362e-01,\n",
       "          1.40691578e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]],\n",
       " \n",
       "        [[9.24033284e-01, 1.75150018e-02, 4.11768891e-02, 4.70066769e-03,\n",
       "          1.25742713e-02],\n",
       "         [0.00000000e+00, 7.65895724e-01, 1.13504075e-01, 8.40537772e-02,\n",
       "          3.65463533e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 8.38960528e-01, 1.07654214e-01,\n",
       "          5.33852167e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.53162074e-01,\n",
       "          4.68378775e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]],\n",
       " \n",
       "        [[9.73589659e-01, 1.44029455e-02, 9.85875167e-03, 9.39828868e-04,\n",
       "          1.20880269e-03],\n",
       "         [0.00000000e+00, 9.87268031e-01, 2.13529379e-03, 9.61607322e-03,\n",
       "          9.80593497e-04],\n",
       "         [0.00000000e+00, 0.00000000e+00, 5.36062658e-01, 3.29917401e-01,\n",
       "          1.34019911e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.85014975e-01,\n",
       "          1.49850436e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]]], dtype=float32)>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, attention = ScaledDotProductAttention()(q=q, k=k, v=v, mask=get_decoder_mask(q))\n",
    "output, attention # both have shape (batch_size, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (training:bool=True,\n",
       ">                                 attention_dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| attention_dropout | float | 0.0 | Will be ignored if `training=False` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (training:bool=True,\n",
       ">                                 attention_dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| attention_dropout | float | 0.0 | Will be ignored if `training=False` |"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ScaledDotProductAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "A small detour to illustrate the softmax function. \n",
    "\n",
    "The $i^{\\text{th}}$ element of $\\text{Softmax}(x)$, with $x \\in \\mathbf{R}^K$ is:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, see the values below for an input vector $x$ ($K=5$ in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [-inf  -1.   0.   1.   3.]\n",
      "exp(x) =  [ 0.          0.36787944  1.          2.71828183 20.08553692]\n",
      "denominator (sum of exp(x_j), j=1,...,K) =  24.171698192818155\n",
      "softmax(x) =  [0.         0.01521943 0.0413707  0.11245721 0.83095266]\n",
      "sum of softmax(x)_j, j=1,...,K =  1.0\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "x = np.array([-np.Inf, -1., 0., 1., 3.])\n",
    "keras.layers.Activation('softmax')(x)\n",
    "print(\"x = \", x)\n",
    "print(\"exp(x) = \", np.exp(x))\n",
    "print(\"denominator (sum of exp(x_j), j=1,...,K) = \", sum(np.exp(x)))\n",
    "print(\"softmax(x) = \", np.exp(x) / sum(np.exp(x)))\n",
    "print(\"sum of softmax(x)_j, j=1,...,K = \", sum(np.exp(x) / sum(np.exp(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the softmax function really makes the largest numbers stand out from the rest.\n",
    "\n",
    "Note also that $-\\infty$ results in 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Multi-head attention\n",
    "\n",
    "* When values are shared in each head and then are aggregated additively, each head still lcan learn different temporal patterns (from their own unique queries and keys), but with the same input values.\n",
    "    * In other words, they can be interpreted as an ensemble over the attention weights\n",
    "    * the paper doesn't mention this explicitly, but the ensemble is equally-weighted - maybe there is some performance to be gained by having some way to weight the different attention heads 🤔, such as having a linear layer combining them... will explore in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadAttention():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head:int,\n",
    "        d_model:int,\n",
    "        training:bool=True, # Whether the layer is being trained or used in inference\n",
    "        dropout:float=0.0 # Will be ignored if `training=False`\n",
    "    ):\n",
    "        self.n_head = n_head\n",
    "        self.d_k = self.d_v = d_k = d_v = d_model // n_head # the original model divides by number of heads\n",
    "        self.training = training\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # using the same value layer facilitates interpretability\n",
    "        vs_layer = keras.layers.Dense(d_v, use_bias=False, name=\"Shared value\")\n",
    "\n",
    "        # creates list of queries, keys and values across heads\n",
    "        self.qs_layers = self._build_layers(d_k, n_head)\n",
    "        self.ks_layers = self._build_layers(d_k, n_head)\n",
    "        self.vs_layers = [vs_layer for _ in range(n_head)]\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.w_o = keras.layers.Dense(d_v, use_bias=False, name=\"W_v\") # W_v in Eqs. (14)-(16), output weight matrix to project internal state to the original TFT\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q, # Queries, tensor of shape (?, time, D_model)\n",
    "        k, # Keys, tensor of shape (?, time, D_model)\n",
    "        v, # Values, tensor of shape (?, time, D_model)\n",
    "        mask=None # Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)\n",
    "    ):\n",
    "        heads = []\n",
    "        attns = []\n",
    "        for i in range(self.n_head):\n",
    "            qs = self.qs_layers[i](q)\n",
    "            ks = self.ks_layers[i](q)\n",
    "            vs = self.vs_layers[i](v)\n",
    "           \n",
    "            head, attn = self.attention(qs, ks, vs, mask)\n",
    "            if self.training:\n",
    "                head = keras.layers.Dropout(self.dropout)(head)\n",
    "            heads.append(head)\n",
    "            attns.append(attn)\n",
    "\n",
    "        outputs = keras.ops.mean(heads, axis=0) if self.n_head > 1 else head # H_tilde\n",
    "        outputs = self.w_o(outputs)\n",
    "        if self.training:\n",
    "            outputs = keras.layers.Dropout(self.dropout)(outputs)\n",
    "\n",
    "        return outputs, attn\n",
    "\n",
    "    def _build_layers(self, d:int, n_head:int):\n",
    "            return [keras.layers.Dense(d) for _ in range(n_head)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "imha = InterpretableMultiHeadAttention(n_head=8, d_model=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 17])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grn.shape # B, T, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_decoder_mask(grn)\n",
    "mask.shape # B, T, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 0.17549314, -0.6499947 ],\n",
       "        [ 1.8635427 , -1.4109019 ],\n",
       "        [ 0.5420394 , -1.9807158 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[0.00578043, 0.15875775, 0.20819457, 0.622656  , 0.00461125],\n",
       "         [0.        , 0.16917567, 0.1295054 , 0.6177507 , 0.08356823],\n",
       "         [0.        , 0.        , 0.03082285, 0.9643422 , 0.00483495],\n",
       "         [0.        , 0.        , 0.        , 0.43220872, 0.56779134],\n",
       "         [0.        , 0.        , 0.        , 0.        , 1.        ]],\n",
       " \n",
       "        [[0.0467045 , 0.85227525, 0.00620111, 0.0769116 , 0.01790758],\n",
       "         [0.        , 0.33351904, 0.46886626, 0.10064775, 0.09696695],\n",
       "         [0.        , 0.        , 0.9063379 , 0.01591888, 0.07774319],\n",
       "         [0.        , 0.        , 0.        , 0.74415594, 0.25584403],\n",
       "         [0.        , 0.        , 0.        , 0.        , 1.        ]],\n",
       " \n",
       "        [[0.10995895, 0.51384526, 0.00828304, 0.3354729 , 0.03243987],\n",
       "         [0.        , 0.26317504, 0.15520625, 0.31223023, 0.26938847],\n",
       "         [0.        , 0.        , 0.6736127 , 0.13027713, 0.19611019],\n",
       "         [0.        , 0.        , 0.        , 0.796808  , 0.203192  ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 1.        ]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imha(grn, grn, grn, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to build the TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalFusionTransformer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Data params\n",
    "        time_steps:int,\n",
    "        input_size:int,\n",
    "        output_size:int,\n",
    "        category_counts:int,\n",
    "        n_workers:int, # Number of multiprocessing workers\n",
    "\n",
    "        # TFT params\n",
    "        input_obs_loc,\n",
    "        static_input_loc,\n",
    "        known_regular_input_idx,\n",
    "        known_categorical_input_idx,\n",
    "        column_definition,\n",
    "\n",
    "        # Network params\n",
    "        quantile:list=[0.1, 0.5, 0.9], # List of quantiles the model should forecast\n",
    "        hidden_layer_size:int=30, # Size of hidden layer\n",
    "        dropout_ratio:float=0.0, # Dropout ratio (between 0.0, inclusive, and less than 1.0)\n",
    "        num_encoder_steps:int=4,\n",
    "        num_stacks:int=4,\n",
    "        num_heads:int=4,\n",
    "        \n",
    "        # Training params\n",
    "        max_gradient_norm:float=1.0, # \n",
    "        learning_rate:float=0.001,\n",
    "        minibatch_size:int=64,\n",
    "        num_epochs:int=100,\n",
    "        early_stopping_patience:int=5,\n",
    "        use_gpu:bool=True\n",
    "    ):\n",
    "        self.time_steps = time_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # Number of periods to be forecasted\n",
    "        self.category_counts = category_counts\n",
    "        self.n_workers = n_workers # Number of multiprocessing workers\n",
    "        \n",
    "        self.input_obs_loc = input_obs_loc\n",
    "        self.static_input_loc = static_input_loc\n",
    "        self.known_regular_input_idx = known_regular_input_idx\n",
    "        self.known_categorical_input_idx = known_categorical_input_idx\n",
    "        self.column_definition = column_definition\n",
    "\n",
    "        self.quantile = quantile # List of quantiles the model should forecast\n",
    "        self.hidden_layer_size = hidden_layer_size # Size of hidden layer\n",
    "        self.dropout_ratio = dropout_ratio # Dropout ratio (between 0.0, inclusive, and less than 1.0)\n",
    "        self.num_encoder_steps = num_encoder_steps\n",
    "        self.num_stacks = num_stacks\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.learning_rate = learning_rate\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def __static_combine_and_mask(\n",
    "        self, \n",
    "        embedding # Transformed static inputs\n",
    "    ):\n",
    "        # Return the tensor output for the variable selection network\n",
    "        # In the paper, that would be the bottom right square in Fig.2\n",
    "\n",
    "        # Add temporal features\n",
    "        _, num_static, _ = embedding.get_shape().as_list() # (embeddings are $\\xi_t^(1, \\dots, \\m_{\\chi})$)\n",
    "        flattened = keras.layers.Flatten()(embedding) # $\\Xi_t$\n",
    "\n",
    "        # Nonlinear transformation with the GRN\n",
    "        mlp_outputs = gated_residual_network(\n",
    "            x=flattened, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=num_static, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "            additional_context=None, # Additional context vector to use if relevant\n",
    "        ) \n",
    "        sparse_weights = keras.layers.Activation('softmax')(mlp_outputs)\n",
    "        sparse_weights = keras.ops.expand_dims(sparse_weights, axis=-1) # $\\upsilon_{\\chi t}$\n",
    "        # it's the sparse weights above that determine by how much the variable will be influencing the model\n",
    "\n",
    "        transformed_embeddings = []\n",
    "        for i in range(num_static):\n",
    "            transformed_embeddings.append(gated_residual_network(\n",
    "                x=embedding[:, i:i+1, :], # Network inputs\n",
    "                hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "                output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "                dropout_rate=self.dropout_rate, # Dropout rate\n",
    "                use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "            ))\n",
    "        transformed_embedding = keras.ops.concatenate(transformed_embeddings, axis=1) # $\\tilde{\\xi_t^(1, \\dots, \\m_{\\chi})}$\n",
    "\n",
    "        combined = keras.layers.Multiply()(\n",
    "            [sparse_weights, transformed_embedding]\n",
    "        )\n",
    "        \n",
    "        static_vec = keras.ops.sum(combined, axis=1)\n",
    "\n",
    "        return static_vec, sparse_weights\n",
    "\n",
    "    def __lstm_combine_and_mask(\n",
    "        self,\n",
    "        embedding # Transformed inputs\n",
    "    ):\n",
    "        # Temporal variable selection networks\n",
    "\n",
    "        # Add temporal features\n",
    "        _, time_steps, embedding_dim, num_inputs = embedding.get_shape().as_list()\n",
    "\n",
    "        flattened = keras.ops.reshape(\n",
    "            embedding,\n",
    "            [1, time_steps, embedding_dim * num_inputs]\n",
    "        )\n",
    "        expanded_static_context_c_s = keras.ops.expand_dims(\n",
    "            self.static_context_variable_selection,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Variable selection weights\n",
    "        mlp_outputs, static_gate = gated_residual_network(\n",
    "            x=flatten,\n",
    "            hidden_layer=self.hidden_layer_size,\n",
    "            output_size=num_inputs,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_time_distributed=True,\n",
    "            additional_context=expanded_static_context_c_s,\n",
    "            return_gate=True\n",
    "        )\n",
    "        sparse_weights = keras.layers.Activation('softmax')(mlp_outputs)\n",
    "        sparse_weights = keras.ops.expand_dims(sparse_weights, axis=2)\n",
    "\n",
    "        # Nonlinear processing and application of weights\n",
    "        transformed_embeddings = []\n",
    "        for i in range(num_inputs):\n",
    "            transformed.embeddings.append(\n",
    "                gated_residual_network(\n",
    "                    embedding[Ellipsis, i],\n",
    "                    hidde_layer=self.hidden_layer_size,\n",
    "                    dropout_rate=self.dropout_rate,\n",
    "                    use_time_distributed=True\n",
    "                )\n",
    "            )\n",
    "        transformed_embeddings = keras.ops.stack(transformed_embeddings, axis=-1)\n",
    "\n",
    "        combined = keras.layers.Multiply()([\n",
    "            sparse_weights, transformed_embedding\n",
    "        ])\n",
    "        temporal_ctx = keras.ops.sum(combined, axis=-1)\n",
    "\n",
    "        return temporal_ctx, sparse_weights, static_gate\n",
    "\n",
    "    def ___convert_real_to_embedding(\n",
    "        self,\n",
    "        x # Inputs\n",
    "    ):\n",
    "        return keras.layers.TimeDistributed(\n",
    "            keras.layers.Dense(self.hidden_layer_size)(x)\n",
    "        )\n",
    "\n",
    "    def __get_tft_embeddings(\n",
    "        self,\n",
    "        all_inputs\n",
    "    ):\n",
    "        # Transform raw inputs to embeddings\n",
    "        # For continuous variables: linear transformation\n",
    "        # For categorical variables: embeddings\n",
    "        \n",
    "        num_categorical_variables = len(self.category_counts)\n",
    "        num_regular_variables = self.input_size - num_categorical_variables\n",
    "\n",
    "        embedding_sizes = [\n",
    "            self.hidden_layer_size\n",
    "            for i, size in enumerate(self.category_counts)\n",
    "        ]\n",
    "\n",
    "        embeddings = [\n",
    "            keras.Sequential([\n",
    "                keras.layers.InputLayer([time_steps]),\n",
    "                keras.layers.Embedding(\n",
    "                    self.category_counts[i],\n",
    "                    embedding_sizes[i],\n",
    "                    input_length=time_steps,\n",
    "                    dtype='float32'\n",
    "                )\n",
    "            ])\n",
    "            for i in range(num_categorical_variables)\n",
    "        ]\n",
    "\n",
    "        regular_inputs, categorical_inputs = \\\n",
    "            all_inputs[:, :, :num_regular_varaibles], \\\n",
    "            all_inputs[:, :, num_regular_variables:]\n",
    "\n",
    "        embedded_inputs = [\n",
    "            embeddings[i](categorical_inputs[Ellipsis, i])\n",
    "            for i in range(num_categorical_variables)\n",
    "        ]\n",
    "\n",
    "        # static inputs\n",
    "        if self._static_input_loc:\n",
    "            st_inp_dense = [\n",
    "                keras.layers.Dense(self.hidden_layer_size)(\n",
    "                    regular_inputs[:, 0, i:i + 1]\n",
    "                )\n",
    "                for i in range(num_regular_variables)\n",
    "                if i in self._static_input_loc\n",
    "            ]\n",
    "            st_inp_embed = [\n",
    "                embedded_inputs[i][:, 0, :]\n",
    "                for i in range(num_categorical_variables)\n",
    "                if  i + num_regular_variables in self._static_input_loc\n",
    "            ]\n",
    "            static_inputs = st_inp_dense + st_inp_embed\n",
    "        else:\n",
    "            static_inputs = None\n",
    "\n",
    "        # Targets\n",
    "        obs_inputs = keras.ops.stack([\n",
    "            self.___convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1])\n",
    "        ], axis=-1)\n",
    "\n",
    "        # past inputs: observed but not known a priori\n",
    "        wired_embeddings = [\n",
    "            embeddings[i](categorical_inputs[:,:,i])\n",
    "            for i in range(num_categorical_variables)\n",
    "            if i not in self._known_categorical_input_idx \\\n",
    "                and i + num_regular_variables not in self._input_obs_loc    \n",
    "        ]\n",
    "        unknown_inputs = [\n",
    "            self.___convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1])\n",
    "            for i in range(regular_inputs.shape[-1])\n",
    "            if i not in self._known_categorical_input_idx \\\n",
    "                and i + num_regular_variables not in self._input_obs_loc    \n",
    "        ]\n",
    "        if wired_embeddings + unknown_inputs:\n",
    "            unknown_inputs = keras.ops.stack(wired_embeddings + unknown_inputs, axis=-1)\n",
    "        else:\n",
    "            unkown_inputs = None\n",
    "\n",
    "        # a priori known inputs\n",
    "        known_regular_inputs = [\n",
    "            self.___convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1])\n",
    "            for i in self._known_regular_input_idx\n",
    "            if i not in self._static_input_loc\n",
    "        ]\n",
    "        known_categorical_inputs = [\n",
    "            embedded_inputs[i]\n",
    "            for i in self._known_categorical_input_idx\n",
    "            if i + num_regular_variables not in self._static_input_loc\n",
    "        ]\n",
    "        known_combined_layer = keras.ops.stack(\n",
    "            known_regular_inputs + known_categorical_inputs,\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        return unknown_inputs, known_combined_layer, past_inputs, static_inputs\n",
    "\n",
    "    def _build_base_graph(self):\n",
    "        # Build the graph, defining the layers of the TFT\n",
    "        \n",
    "        all_inputs = keras.layers.Input(\n",
    "            shape=(self.time_steps, self.input_size)\n",
    "        )\n",
    "        unknown_inputs, known_combined_layer, past_inputs, static_inputs \\\n",
    "            = self.__get_tft_embeddings(all_inputs)\n",
    "\n",
    "        # first we isolate the known future inputs and observed past inputs\n",
    "        if unknown_inputs is not None:\n",
    "            historical_inputs = keras.ops.concatenate([\n",
    "                unknown_inputs[:, :self.num_encoder_steps, :],\n",
    "                known_combined_layer[:, :self.num_encoder_steps, :],\n",
    "                past_inputs[:, :self.num_encoder_steps, :]\n",
    "            ], axis=1)\n",
    "        else:\n",
    "            historical_inputs = keras.ops.concatenate([\n",
    "                known_combined_layer[:, :self.num_encoder_steps, :],\n",
    "                past_inputs[:, :self.num_encoder_steps, :]\n",
    "            ])\n",
    "        \n",
    "        # and then we isolate the known future inputs\n",
    "        future_inputs = known_combined_layer[:, :self.num_encoder_steps, :]\n",
    "\n",
    "        # static vars\n",
    "        static_encoder, static_weights = self.__static_combine_and_mask(static_inputs)\n",
    "\n",
    "        # Static covariate encoders\n",
    "        # These integrate static features into the network through encoding of context vectors\n",
    "        # that condition the time-varying dynamics\n",
    "        self.static_context_variable_selection = gated_residual_network( # c_s\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "        )\n",
    "        self.static_context_enrichment = gated_residual_network( # c_3\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "        )\n",
    "        self.static_context_state_h = gated_residual_network( # c_h\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "        )\n",
    "        self.static_context_state_c = gated_residual_network( # c_c\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "        )\n",
    "\n",
    "        historical_features, historical_flags, _ = self.__lstm_combine_and_mask(historical_inputs)\n",
    "        future_features, future_flags, _ = self.__lstm_combine_and_mask(future_inputs)\n",
    "\n",
    "        # Locality enhancement (Section 4.5.1 in paper) with seq-to-seq layer\n",
    "\n",
    "        # LSTM layers: LSTM Encoder for encoding past inputs\n",
    "        history_lstm, state_h, state_c = keras.layers.LSTM(\n",
    "            units=self.hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            stateful=False,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            recurrent_dropout=0,\n",
    "            unroll=False,\n",
    "            use_bias=True)(\n",
    "                inputs=historical_features,\n",
    "                initial_state=[\n",
    "                    self.static_context_state_h, # short-term state\n",
    "                    self.static_context_state_c  # long-term state\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # LSTM layers: LST Decoder for decoding future inputs\n",
    "        future_lstm = keras.layers.LSTM(\n",
    "            units=self.hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=False,\n",
    "            stateful=False,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            recurrent_dropout=0,\n",
    "            unroll=False,\n",
    "            use_bias=True)(\n",
    "                inputs=future_features,\n",
    "                initial_stage=[\n",
    "                    state_h, # short-term state\n",
    "                    state_c  # long-term state\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        lstm_layer = keras.ops.concatenate([history_lstm, future_lstm], axis=1)\n",
    "\n",
    "        # Apply gated skip connection (Gate followed by Add & Norm)\n",
    "\n",
    "        input_embeddings = keras.ops.concatenate(\n",
    "            [historical_features, future_features],\n",
    "            axis=1\n",
    "        )\n",
    "        lstm_layer, _ = apply_gating_layer(\n",
    "            x=lstm_layer, # Input tensors (batch first)\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GLU\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=True, # Apply the GLU across all timesteps?\n",
    "            activation=None # Activation function\n",
    "        )\n",
    "        temporal_feature_layer = add_and_norm([lstm_layer, input_embeddings])\n",
    "\n",
    "        # Temporal Fusion Decoder (TFT, Purple box in Fig. 2)\n",
    "        # contains three steps\n",
    "        # TFT 1st step: Static enrichment\n",
    "        #   - enhances the temporal features with static metadata (Eq. 18)\n",
    "        \n",
    "        expanded_static_context_c_e = keras.ops.expand_dims(\n",
    "            static_context_enrichment,\n",
    "            axis=1\n",
    "        )\n",
    "        enriched, _ = gated_residual_network( # $\\theta(t, n) = \\text{GRN}_{\\theta}(\\tilde{\\theta}(t, n), c_e)\n",
    "            x=temporal_feature_layer,\n",
    "            hidden_layer_size=self.hidden_layer_size,\n",
    "            output_size=self.hidden_layer_size,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_time_distributed=True,\n",
    "            additional_context=expanded_static_context_c_e,\n",
    "            return_gate=True\n",
    "        )\n",
    "\n",
    "        # TFT 2nd step: Temporal self-attention\n",
    "\n",
    "        self_attention_layer = InterpretableMultiHeadAttention(\n",
    "            n_head=self.num_heads,\n",
    "            d_model=self.hidden_layer_size,\n",
    "            dropout=self.dropout_rate # Will be ignored if `training=False`\n",
    "        )\n",
    "        mask = get_decoder_mask(enriched)\n",
    "        post_attn, self_attention = self_attention_layer( # $B(t) = \\text{IMHA}(\\Theta(t), \\Theta(t), \\Theta(t))$\n",
    "            q=enriched,\n",
    "            k=enriched,\n",
    "            v=enriched,\n",
    "            mask=mask\n",
    "        )\n",
    "        post_attn, _ = apply_gating_layer( # $\\text{GLU}_{\\delta}(\\beta(t, n))$\n",
    "            x=post_attn, # Input tensors (batch first)\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GLU\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=True, # Apply the GLU across all timesteps?\n",
    "            activation=None # Activation function\n",
    "        )\n",
    "        post_attn = add_and_norm([post_attn, enriched]) # \\delta(t, n) = \\text{LayerNorm}(\\theta(t, n) + $\\text{GLU}_{\\delta}(\\beta(t, n)))$\n",
    "\n",
    "        # TFT 3rd step: Position-wise feed-forward\n",
    "        decoder = gated_residual_network(\n",
    "            x=post_attn,\n",
    "            hidden_layer_size=self.hidden_layer_size,\n",
    "            output_size=self.hidden_layer_size,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_time_distributed=True,\n",
    "            additional_context=None,\n",
    "            return_gate=False\n",
    "        )\n",
    "\n",
    "        # final skip connection\n",
    "        decoder, _ = apply_gating_layer(\n",
    "            x=decoder, # Input tensors (batch first)\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GLU\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            activation=None # Activation function\n",
    "        )\n",
    "        transformer_layer = add_and_norm([decoder, temporal_future_layer])\n",
    "\n",
    "        # the function also returns the attention components\n",
    "        # for explainability analyses\n",
    "        attention_components = {\n",
    "            \"temporal_attention_weights\": self_attention,\n",
    "            \"variable_selection_weights_static_inputs\": static_weights[Ellipsis, 0],\n",
    "            \"variable_selection_weights_past_inputs\": historical_flags[Ellipsis, 0, :],\n",
    "            \"variable_selection_weights_future_inputs\": future_flags[Ellipsis, 0, :]\n",
    "        }\n",
    "\n",
    "        return transformer_layer, all_inputs, attention_components\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build model and define training losses\n",
    "\n",
    "        transformer_layer, all_inputs, self._attention_components = self._build_base_graph()\n",
    "        outputs = keras.layers.TimeDistributed(\n",
    "            keras.layers.Dense(self.output_size * len(self.quantiles))\n",
    "        )(transformer_layer[Ellipsis, self.num_encoder_steps:, :])\n",
    "        model = keras.Model(inputs=all_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb Cell 65\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tft \u001b[39m=\u001b[39m TemporalFusionTransformer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     time_steps\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     input_size\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     output_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     category_counts\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     n_workers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m# Number of multiprocessing workers\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# TFT params\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     input_obs_loc\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     static_input_loc\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     known_regular_input_idx\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     known_categorical_input_idx\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     column_definition\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb Cell 65\u001b[0m in \u001b[0;36mTemporalFusionTransformer.__init__\u001b[0;34m(self, time_steps, input_size, output_size, category_counts, n_workers, input_obs_loc, static_input_loc, known_regular_input_idx, known_categorical_input_idx, column_definition, quantile, hidden_layer_size, dropout_ratio, num_encoder_steps, num_stacks, num_heads, max_gradient_norm, learning_rate, minibatch_size, num_epochs, early_stopping_patience, use_gpu)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mearly_stopping_patience \u001b[39m=\u001b[39m early_stopping_patience\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_gpu \u001b[39m=\u001b[39m use_gpu\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_model()\n",
      "\u001b[1;32m/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb Cell 65\u001b[0m in \u001b[0;36mTemporalFusionTransformer.build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=451'>452</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=452'>453</a>\u001b[0m     \u001b[39m# Build model and define training losses\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=454'>455</a>\u001b[0m     transformer_layer, all_inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attention_components \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_base_graph()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=455'>456</a>\u001b[0m     outputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mTimeDistributed(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=456'>457</a>\u001b[0m         keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantiles))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=457'>458</a>\u001b[0m     )(transformer_layer[\u001b[39mEllipsis\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_encoder_steps:, :])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=458'>459</a>\u001b[0m     model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39mall_inputs, outputs\u001b[39m=\u001b[39moutputs)\n",
      "\u001b[1;32m/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb Cell 65\u001b[0m in \u001b[0;36mTemporalFusionTransformer._build_base_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=260'>261</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build_base_graph\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=261'>262</a>\u001b[0m     \u001b[39m# Build the graph, defining the layers of the TFT\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=263'>264</a>\u001b[0m     all_inputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mInput(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=264'>265</a>\u001b[0m         shape\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_steps, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m     unknown_inputs, known_combined_layer, past_inputs, static_inputs \\\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m         \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_tft_embeddings(all_inputs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m     \u001b[39m# first we isolate the known future inputs and observed past inputs\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m     \u001b[39mif\u001b[39;00m unknown_inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb Cell 65\u001b[0m in \u001b[0;36mTemporalFusionTransformer.__get_tft_embeddings\u001b[0;34m(self, all_inputs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_tft_embeddings\u001b[39m(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m     all_inputs\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m     \u001b[39m# For continuous variables: linear transformation\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m     \u001b[39m# For categorical variables: embeddings\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m     num_categorical_variables \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategory_counts)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m     num_regular_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m-\u001b[39m num_categorical_variables\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m     embedding_sizes \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layer_size\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i, size \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategory_counts)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m     ]\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer(\n",
    "    time_steps=12,\n",
    "    input_size=20,\n",
    "    output_size=4,\n",
    "    category_counts=5,\n",
    "    n_workers=2, # Number of multiprocessing workers\n",
    "\n",
    "    # TFT params\n",
    "    input_obs_loc=24,\n",
    "    static_input_loc=24,\n",
    "    known_regular_input_idx=24,\n",
    "    known_categorical_input_idx=24,\n",
    "    column_definition=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft.time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# References {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv_gingado')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977c2d9435ad3a481cf1bbece8d5ecb19e078de55648a0a0bad32b79c2e18340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
