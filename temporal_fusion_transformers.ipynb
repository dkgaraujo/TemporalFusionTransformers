{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Experimenting with TFT\"\n",
    "author: Douglas Araujo\n",
    "format: \n",
    "    html:\n",
    "        toc: true\n",
    "        toc-location: right\n",
    "        number-sections: true\n",
    "        code-fold: true\n",
    "        code-tools: true\n",
    "bibliography: ref.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the temporal fusion transformers [@lim2021temporal] architecture, and ports it over to keras 3 while making some punctual improvements.\n",
    "\n",
    "The original repository is: https://github.com/google-research/google-research/tree/master/tft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 21:35:34.703677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import keras_core as keras\n",
    "from keras_core import layers\n",
    "from fastcore import docments\n",
    "from nbdev.showdoc import show_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **time distributed**: \n",
    "  * applies same layer to each of the timesteps in the data\n",
    "    * in other words, a layer with the exact same weights\n",
    "  * indices:\n",
    "    * index 0: batch\n",
    "    * index 1: time\n",
    "    * indices 2...: data\n",
    "  * More info: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated residual network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer\n",
    "\n",
    "* dedicated implementation to better control use of time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(size:int, # Output size\n",
    "                 activation:str|callable|None=None, # Activation function\n",
    "                 use_time_distributed:bool=False, # Apply the layer across all timesteps?\n",
    "                 use_bias:bool=True # Include bias in the layer?\n",
    ")->keras.src.layers.core.dense.Dense: # Dense layer\n",
    "    \"Linear layer.\"\n",
    "\n",
    "    linear = keras.layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "    if use_time_distributed:\n",
    "        linear = keras.layers.TimeDistributed(linear)\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layer\n",
    "\n",
    "* dedicated implementation to better control use of time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(\n",
    "    size:int, # Output size\n",
    "    activation:str|callable|None=None, # Activation function\n",
    "    use_time_distributed:bool=False, # Apply the layer across all timesteps?\n",
    "    use_bias:bool=True # Include bias in the layer?\n",
    ")->keras.src.layers.core.dense.Dense: # Dense layer\n",
    "    \"Dense layer\"\n",
    "\n",
    "    dense = layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "    if use_time_distributed:\n",
    "        dense = layers.TimeDistributed(dense)\n",
    "    return dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 21:35:38.539842: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-11 21:35:38.539886: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "layer_size = 16\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# dense layer\n",
    "dense = dense_layer(size=layer_size, use_time_distributed=True)\n",
    "\n",
    "# output dimensions: batches / timesteps / layer size\n",
    "assert dense(x).shape == [batch_size, n_timesteps, layer_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now showing that the time-distributed layer applies the same weights at all timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "x = np.ones((1, n_timesteps, n_features))\n",
    "timesteps_equal = []\n",
    "for i in range(n_timesteps-1):\n",
    "    timesteps_equal.append((np.array_equal(dense(x)[0,0,:], dense(x)[0,i+1,:])))\n",
    "\n",
    "assert np.all(timesteps_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### dense_layer\n",
       "\n",
       "\n",
       "\n",
       "Dense layer"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### dense_layer\n",
       "\n",
       "\n",
       "\n",
       "Dense layer"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "show_doc(dense_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated linear unit (GLU)\n",
    "\n",
    "* Introduced by @dauphin2017language\n",
    "* The GLU is part of the Gated Residual Network (GRN) block\n",
    "* Using input $\\gamma \\in \\mathbb{R}^{d_{\\text{model}}}$ and the subscript $\\omega$ to index weights, $\\text{GLU}_{\\omega}(\\gamma) = \\sigma(W_{4, \\omega} \\gamma + b_{4, \\omega}) \\odot (W_{5, \\omega} \\gamma + b_{5, \\omega})$\n",
    "* As can be seen above, the result could be very close to zero through the Hadamard multipliciation, which in practice means that the network would not be affected by that data (ie, it would be gated out)\n",
    "* *\"GLUs reduce the vanishing gradient problem for deep architectures by providing a linear path for gradients while retaining non-linear capabilities\"*\n",
    "* *\"provide flexibility to suppress any parts of the architecture that are not required for a given dataset\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "\n",
    "def apply_gating_layer(\n",
    "    x, # Input tensors (batch first)\n",
    "    hidden_layer_size:int, # Dimension of the GLU\n",
    "    dropout_rate:float|None=None, # Dropout rate\n",
    "    use_time_distributed:bool=True, # Apply the GLU across all timesteps?\n",
    "    activation:str|callable=None # Activation function\n",
    "): # Tuple of (GLU output tensors, gated_layer)\n",
    "    \"Gated Linear Unit (GLU) layer\"\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    activation_layer = layers.Dense(\n",
    "                hidden_layer_size,\n",
    "                activation=activation\n",
    "            )\n",
    "\n",
    "    gated_layer = layers.Dense(\n",
    "                hidden_layer_size,\n",
    "                activation='sigmoid'\n",
    "            )\n",
    "\n",
    "    if use_time_distributed:\n",
    "        activation_layer = layers.TimeDistributed(activation_layer)(x)\n",
    "        gated_layer = layers.TimeDistributed(gated_layer)(x)\n",
    "    else:\n",
    "        activation_layer = activation_layer(x)\n",
    "        gated_layer = gated_layer(x)\n",
    "\n",
    "    return layers.Multiply()([activation_layer, gated_layer]), gated_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of GLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "hidden_layer_size = 16\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# output dimensions: batches / timesteps / hidden_layer_size\n",
    "assert apply_gating_layer(x=x, hidden_layer_size=hidden_layer_size)[0].shape == [batch_size, n_timesteps, hidden_layer_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### apply_gating_layer\n",
       "\n",
       "\n",
       "\n",
       "Gated Linear Unit (GLU) layer"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### apply_gating_layer\n",
       "\n",
       "\n",
       "\n",
       "Gated Linear Unit (GLU) layer"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "show_doc(apply_gating_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip connection\n",
    "\n",
    "Adds inputs to layer, ie \"skip connection\", and then implements layer normalisation [@ba2016layer]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_norm(\n",
    "    x_list # List of input tensors (of the same dimension) for skip connection\n",
    "    ):\n",
    "    \"Adds tensors with same dimensions and then normalises layer\"\n",
    "    tmp = layers.Add()(x_list)\n",
    "    return layers.LayerNormalization()(tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of skip connections + layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x1 = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "x2 = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# output dimensions: batches / timesteps / features\n",
    "x1x2 = add_and_norm(x_list=[x1, x2])\n",
    "assert x1.shape == x1x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean values (normalised should be around 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.21064889,  0.00084476,  0.11828817,  0.10758099,  0.13766904],\n",
       "        [-0.10135074, -0.07045425,  0.14023677, -0.11775093, -0.02139493],\n",
       "        [ 0.19945069,  0.15204023, -0.00915458, -0.05093143,  0.06814506]]),\n",
       " array([[-0.04798872, -0.03549833,  0.03819124,  0.06419377,  0.0571289 ],\n",
       "        [ 0.03671484,  0.01313728,  0.24330115, -0.02981159, -0.13643608],\n",
       "        [ 0.16745957,  0.15059631,  0.12545591,  0.04036627,  0.221975  ]]),\n",
       " array([[-2.3841857e-09,  3.5762786e-09, -3.5762786e-09,  1.9669534e-08,\n",
       "          2.3841857e-09],\n",
       "        [ 2.7418137e-08, -3.3974647e-08,  9.0897085e-09, -1.0430813e-08,\n",
       "          8.3446503e-09],\n",
       "        [-1.3113022e-08, -1.5497207e-08, -2.0861625e-08,  5.9604643e-10,\n",
       "         -1.8775463e-08]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.mean(axis=-1), x2.mean(axis=-1), x1x2.numpy().mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation (normalised should be around 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.95521739, 1.01235777, 1.06807431, 1.07700502, 1.00896732],\n",
       "        [0.98399389, 0.96555043, 0.97562998, 0.91029426, 1.12211808],\n",
       "        [0.90803724, 0.98626599, 0.95183459, 0.95716115, 1.00375778]]),\n",
       " array([[0.87385494, 0.97622118, 0.90593613, 0.91305024, 0.9836994 ],\n",
       "        [0.98398415, 0.98993835, 0.99793171, 1.09443932, 1.08308257],\n",
       "        [0.9164024 , 1.12422456, 1.18169786, 0.98498   , 1.0061887 ]]),\n",
       " array([[0.9996749 , 0.9997219 , 0.99976456, 0.9997423 , 0.9997195 ],\n",
       "        [0.99968237, 0.9997152 , 0.9996925 , 0.9997782 , 0.99981135],\n",
       "        [0.99963486, 0.9997779 , 0.99981385, 0.99975884, 0.9997358 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.std(axis=-1), x2.std(axis=-1), x1x2.numpy().std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### add_and_norm\n",
       "\n",
       ">      add_and_norm (x_list)\n",
       "\n",
       "Adds tensors with same dimensions and then normalises layer\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| x_list | List of input tensors (of the same dimension) for skip connection |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### add_and_norm\n",
       "\n",
       ">      add_and_norm (x_list)\n",
       "\n",
       "Adds tensors with same dimensions and then normalises layer\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| x_list | List of input tensors (of the same dimension) for skip connection |"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "\n",
    "show_doc(add_and_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated residual network (GRN)\n",
    "\n",
    "* The GRN is a key building block of the TFT\n",
    "    * Helps keep information only from relevant input variables\n",
    "    * Also keeps the model as simple as possible by only applying non-linearities when relevant\n",
    "* $\\text{GRN}_{\\omega}(a, c)$:\n",
    "    * *1st step*: $\\eta_{2} = \\text{ELU}(W_{2, \\omega} a + b_{2, \\omega} + W_{3, \\omega} c)$, (where the additional context $c$ might be zero),\n",
    "    * *2nd step*: $\\eta_{1} = W_{1, \\omega} \\eta_{2} + b_{1, w}$,\n",
    "    * *3rd step*: $\\text{LayerNorm}(a + \\text{GLU}_{\\omega}(\\eta_{1}))$\n",
    "* $\\text{ELU}(\\cdot)$ is the Exponential Linear Unit activation function (@clevert2015fast)\n",
    "    * Unlike ReLUs, ELUs allow for negative values, which pushes unit activations closer to zero at a lower computation complexity, and producing more accurate results\n",
    "* $\\text{LayerNorm}(\\cdot)$ is the layer normalisation (@ba2016layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_residual_network(\n",
    "    x, # Network inputs\n",
    "    hidden_layer_size:int, # Dimension of the GRN\n",
    "    output_size:int|None=None, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "    dropout_rate:float|None=None, # Dropout rate\n",
    "    use_time_distributed:bool=True, # Apply the GRN across all timesteps?\n",
    "    additional_context=None, # Additional context vector to use if relevant\n",
    "    return_gate:bool=False #Whether to return GRN gate for diagnostic purposes\n",
    "):\n",
    "    \"Applies the gated residual network (GRN) as defined in the paper\"\n",
    "    \n",
    "    # Setup skip connection\n",
    "    if output_size is None:\n",
    "        output_size = hidden_layer_size\n",
    "        skip = x\n",
    "    else:\n",
    "        linear = keras.layers.Dense(output_size)\n",
    "        if use_time_distributed:\n",
    "            linear = keras.layers.TimeDistributed(linear)\n",
    "        skip = linear(x)\n",
    "\n",
    "    # 1st step: eta2\n",
    "    hidden = linear_layer(\n",
    "        size=hidden_layer_size, # W2\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=True # b2\n",
    "    )(x)\n",
    "\n",
    "    # \"For instances without a context vector, the GRN simply treates the context input as zero - ie, $c = 0$ in Eq. 4\"\n",
    "    if additional_context is not None: # if c is != 0...\n",
    "        hidden += linear_layer(\n",
    "            size=hidden_layer_size, # W3\n",
    "            activation=None,\n",
    "            use_time_distributed=use_time_distributed,\n",
    "            use_bias=False # no bias for additional context, since there already is bias from the \"main\" calculation of eta2\n",
    "        )(additional_context)\n",
    "\n",
    "    hidden = keras.layers.Activation('elu')(hidden)\n",
    "\n",
    "    # 2nd step: eta1\n",
    "    hidden = linear_layer(\n",
    "        size=hidden_layer_size, # W1\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=True # b1\n",
    "    )(hidden)\n",
    "\n",
    "    # 3rd step: concluding the GRN calculation\n",
    "    gating_layer, gate = apply_gating_layer(\n",
    "        x=hidden,\n",
    "        hidden_layer_size=output_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        activation=None\n",
    "    )\n",
    "\n",
    "    GRN = add_and_norm([skip, gating_layer])\n",
    "\n",
    "    if return_gate:\n",
    "        return GRN, gate\n",
    "    else:\n",
    "        return GRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of GRN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "hidden_layer_size = 16\n",
    "output_size = 17\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "grn = gated_residual_network(\n",
    "    x=x,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=0,\n",
    "    use_time_distributed=True,\n",
    "    additional_context=None,\n",
    "    return_gate=False\n",
    ")\n",
    "\n",
    "# output dimensions: batches / timesteps / hidden_layer_size\n",
    "assert grn.shape == [batch_size, n_timesteps, output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### gated_residual_network\n",
       "\n",
       ">      gated_residual_network (x, hidden_layer_size:int,\n",
       ">                              output_size:int|None=None,\n",
       ">                              dropout_rate:float|None=None,\n",
       ">                              use_time_distributed:bool=True,\n",
       ">                              additional_context=None, return_gate:bool=False)\n",
       "\n",
       "Applies the gated residual network (GRN) as defined in the paper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Network inputs |\n",
       "| hidden_layer_size | int |  | Dimension of the GRN |\n",
       "| output_size | int \\| None | None | Size of output layer (if None, same as `hidden_layer_size`) |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GLU across all timesteps? |\n",
       "| additional_context | NoneType | None | Additional context vector to use if relevant |\n",
       "| return_gate | bool | False | Whether to return GLU gate for diagnostic purposes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### gated_residual_network\n",
       "\n",
       ">      gated_residual_network (x, hidden_layer_size:int,\n",
       ">                              output_size:int|None=None,\n",
       ">                              dropout_rate:float|None=None,\n",
       ">                              use_time_distributed:bool=True,\n",
       ">                              additional_context=None, return_gate:bool=False)\n",
       "\n",
       "Applies the gated residual network (GRN) as defined in the paper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Network inputs |\n",
       "| hidden_layer_size | int |  | Dimension of the GRN |\n",
       "| output_size | int \\| None | None | Size of output layer (if None, same as `hidden_layer_size`) |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GLU across all timesteps? |\n",
       "| additional_context | NoneType | None | Additional context vector to use if relevant |\n",
       "| return_gate | bool | False | Whether to return GLU gate for diagnostic purposes |"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "\n",
    "show_doc(gated_residual_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention components\n",
    "\n",
    "* Attention mechanisms use relationships between keys $K \\in \\mathbf{R}^{N \\times d_{attention}}$ and queries $Q \\in \\mathbf{R}^{N \\times d_{attention}}$ to scale a vector of values $V \\in \\mathbf{R}^{N \\times d_V}$: $\\text{Attention}(Q, K, V) = A(Q, K) V$\n",
    "    * $N$ is the number of timesteps going into the attention layer (the number of lags plus the number of periods to be forecasted)\n",
    "    * $A(\\cdot)$ is a normalisation function\n",
    "        * After @vaswani2017attention, the canonical choice for $A(\\cdot)$ is the scaled dot-product: $A(Q, K) = \\text{Softmax}(\\frac{Q K^{T}}{\\sqrt{d_{attention}}} )$\n",
    "    \n",
    "* The TFT uses a modified attention head to enhance the explainability of the model\n",
    "* Specifically, the transformer block (multi-head attention) is modified to:\n",
    "    * share values in each head, and\n",
    "    * employ additive aggregation of all heads\n",
    "* More formally, compare the interpretable multi-head attention (used in this paper) with the canonical multi-head attention:\n",
    "    * $\\text{InterpretableMultiHead}(Q, K, V) = \\tilde{H} W_{H}$, with:\n",
    "        * $\\begin{aligned}\\tilde{H} &= \\tilde{A}(Q, K) V W_V \\\\\n",
    "        &= \\{\\frac{1}{m_H} \\sum^{m_{H}}_{h=1} A(Q W^{(h)}_Q, K W^{(h)}_K) \\} V W_V \\\\\n",
    "        &= \\frac{1}{m_H} \\sum^{m_{H}}_{h=1} \\text{Attention}(Q W^{(h)}_Q, K W^{(h)}_K, V W_V)\n",
    "        \\end{aligned}$\n",
    "    * $\\text{MultiHead}(Q, K, V) = [H_1, \\dots, H_{m_H}] W_H$, with:\n",
    "        * $H_h = \\text{Attention}(Q W^{(h)}_Q, K W^{(h)}_K, V W_V^{(h)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder mask for self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_mask(\n",
    "    self_attention_inputs # Inputs to the self-attention layer\n",
    "):\n",
    "    \"Determines shape of decoder mask\"\n",
    "    len_s = keras.ops.shape(self_attention_inputs)[1] # length of inputs\n",
    "    bs = keras.ops.shape(self_attention_inputs)[0] # batch shape\n",
    "    mask = keras.ops.cumsum(keras.ops.eye(len_s), 1) #keras.backend.cumsum(np.eye(len_s, bs))\n",
    "\n",
    "    ### warning: I had to manually implement some batch-wise shape here \n",
    "    ### because the new keras `eye` function does not have a batch_size arg.\n",
    "    ### inspired by: https://github.com/tensorflow/tensorflow/blob/v2.14.0/tensorflow/python/ops/linalg_ops_impl.py#L30\n",
    "    ### <hack>\n",
    "    mask = keras.ops.expand_dims(mask, axis=0)    \n",
    "    mask = keras.ops.tile(mask, (bs, 1, 1))\n",
    "    ### </hack>\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of the decoder mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = get_decoder_mask(grn)\n",
    "\n",
    "assert dec.shape == (batch_size, n_timesteps, n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it produces an upper-triangular matrix of ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_decoder_mask\n",
       "\n",
       ">      get_decoder_mask (self_attention_inputs)\n",
       "\n",
       "Determines shape of decoder mask\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| self_attention_inputs | Inputs to the self-attention layer |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_decoder_mask\n",
       "\n",
       ">      get_decoder_mask (self_attention_inputs)\n",
       "\n",
       "Determines shape of decoder mask\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| self_attention_inputs | Inputs to the self-attention layer |"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_decoder_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention layer\n",
    "\n",
    "* This is the same as Eq. (1) of @vaswani2017attention \n",
    "    * except that in this case the dimension of the value vector is the same $d_{\\text{model}}$ as for the query and key vectors\n",
    "* As discussed in the paper, additive attention outperforms dot product attention for larger $d_{\\text{model}}$ values, so the attention is scaled back to smaller values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention():\n",
    "    def __init__(\n",
    "        self,\n",
    "        training:bool=True, # Whether the layer is being trained or used in inference\n",
    "        attention_dropout:float=0.0 # Will be ignored if `training=False`\n",
    "    ):\n",
    "        self.training = training\n",
    "        self.dropout = keras.layers.Dropout(rate=attention_dropout)\n",
    "        self.activation = keras.layers.Activation('softmax')\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q, # Queries, tensor of shape (?, time, D_model)\n",
    "        k, # Keys, tensor of shape (?, time, D_model)\n",
    "        v, # Values, tensor of shape (?, time, D_model)\n",
    "        mask # Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)\n",
    "    ):\n",
    "        # returns Tuple (layer outputs, attention weights)\n",
    "        scale = keras.ops.sqrt(keras.ops.cast(keras.ops.shape(k)[-1], dtype='float32'))\n",
    "        attention = keras.ops.einsum(\"bij,bjk->bik\", q, keras.ops.transpose(k, axes=(0, 2, 1))) / scale\n",
    "        if mask is not None:\n",
    "            mmask = keras.layers.Lambda(lambda x: (-1e9) * (1. - keras.ops.cast(x, 'float32')))(mask)\n",
    "            attention = keras.layers.Add()([attention, mmask])\n",
    "        attention = self.activation(attention)\n",
    "        if self.training:\n",
    "            attention = self.dropout(attention)\n",
    "        output = keras.ops.einsum(\"btt,btd->bt\", attention, v)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how the `ScaledDotProductAttention` layer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 13\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x_btf = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# using the same vector for q, k and v just to simplify\n",
    "q=keras.ops.cast(x_btf, 'float32')\n",
    "k=keras.ops.cast(x_btf, 'float32')\n",
    "v=keras.ops.cast(x_btf, 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing without masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[-1.3595526 ,  2.3038948 , -0.9351378 , -2.1107376 , -0.10875011],\n",
       "        [ 0.74828297,  1.2987878 ,  2.7947266 ,  3.3696222 ,  0.02006484],\n",
       "        [-4.0625644 ,  7.9136066 ,  0.15763263,  0.387267  ,  0.5300054 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[8.67120624e-01, 7.71935284e-02, 1.09112160e-02, 3.08754463e-02,\n",
       "          1.38991298e-02],\n",
       "         [1.48252323e-01, 7.48296022e-01, 6.03066683e-02, 2.28216760e-02,\n",
       "          2.03233790e-02],\n",
       "         [4.45502140e-02, 1.28209814e-01, 7.00278938e-01, 7.39124417e-02,\n",
       "          5.30485511e-02],\n",
       "         [2.03909084e-01, 7.84784630e-02, 1.19554065e-01, 5.13916612e-01,\n",
       "          8.41417834e-02],\n",
       "         [5.25127314e-02, 3.99808772e-02, 4.90878299e-02, 4.81354706e-02,\n",
       "          8.10283065e-01]],\n",
       " \n",
       "        [[9.24033284e-01, 1.75150018e-02, 4.11768891e-02, 4.70066769e-03,\n",
       "          1.25742713e-02],\n",
       "         [9.32998657e-02, 6.94437802e-01, 1.02914162e-01, 7.62115791e-02,\n",
       "          3.31365839e-02],\n",
       "         [1.35585129e-01, 6.36154786e-02, 6.71839118e-01, 8.62094313e-02,\n",
       "          4.27508503e-02],\n",
       "         [1.26347877e-02, 3.84555049e-02, 7.03727603e-02, 8.37388158e-01,\n",
       "          4.11488079e-02],\n",
       "         [1.19896367e-01, 5.93143813e-02, 1.23796776e-01, 1.45972848e-01,\n",
       "          5.51019549e-01]],\n",
       " \n",
       "        [[9.73589659e-01, 1.44029455e-02, 9.85875167e-03, 9.39828868e-04,\n",
       "          1.20880269e-03],\n",
       "         [5.55578293e-03, 9.81782973e-01, 2.12343060e-03, 9.56264790e-03,\n",
       "          9.75145609e-04],\n",
       "         [2.00642303e-01, 1.12032667e-01, 3.68449330e-01, 2.26760492e-01,\n",
       "          9.21152383e-02],\n",
       "         [1.68646057e-03, 4.44847867e-02, 1.99937429e-02, 9.19841528e-01,\n",
       "          1.39935594e-02],\n",
       "         [4.58808010e-03, 9.59513243e-03, 1.71793364e-02, 2.95989700e-02,\n",
       "          9.39038575e-01]]], dtype=float32)>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "output, attention = ScaledDotProductAttention()(q=q, k=k, v=v, mask=None)\n",
    "output, attention # both have shape (batch_size, n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and with masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[-1.3595526 ,  2.704903  , -1.130431  , -3.5293167 , -0.1342125 ],\n",
       "        [ 0.74828297,  1.4324336 ,  3.4899209 ,  3.8354926 ,  0.03641403],\n",
       "        [-4.0625644 ,  7.9578185 ,  0.22934216,  0.414706  ,  0.56441283]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[8.67120624e-01, 7.71935284e-02, 1.09112160e-02, 3.08754463e-02,\n",
       "          1.38991298e-02],\n",
       "         [0.00000000e+00, 8.78541887e-01, 7.08034411e-02, 2.67939400e-02,\n",
       "          2.38607973e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 8.46524537e-01, 8.93482491e-02,\n",
       "          6.41271621e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.59308362e-01,\n",
       "          1.40691578e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]],\n",
       " \n",
       "        [[9.24033284e-01, 1.75150018e-02, 4.11768891e-02, 4.70066769e-03,\n",
       "          1.25742713e-02],\n",
       "         [0.00000000e+00, 7.65895724e-01, 1.13504075e-01, 8.40537772e-02,\n",
       "          3.65463533e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 8.38960528e-01, 1.07654214e-01,\n",
       "          5.33852167e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.53162074e-01,\n",
       "          4.68378775e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]],\n",
       " \n",
       "        [[9.73589659e-01, 1.44029455e-02, 9.85875167e-03, 9.39828868e-04,\n",
       "          1.20880269e-03],\n",
       "         [0.00000000e+00, 9.87268031e-01, 2.13529379e-03, 9.61607322e-03,\n",
       "          9.80593497e-04],\n",
       "         [0.00000000e+00, 0.00000000e+00, 5.36062658e-01, 3.29917401e-01,\n",
       "          1.34019911e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.85014975e-01,\n",
       "          1.49850436e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]]], dtype=float32)>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, attention = ScaledDotProductAttention()(q=q, k=k, v=v, mask=get_decoder_mask(q))\n",
    "output, attention # both have shape (batch_size, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (training:bool=True,\n",
       ">                                 attention_dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| attention_dropout | float | 0.0 | Will be ignored if `training=False` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (training:bool=True,\n",
       ">                                 attention_dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| attention_dropout | float | 0.0 | Will be ignored if `training=False` |"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ScaledDotProductAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "A small detour to illustrate the softmax function. \n",
    "\n",
    "The $i^{\\text{th}}$ element of $\\text{Softmax}(x)$, with $x \\in \\mathbf{R}^K$ is:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, see the values below for an input vector $x$ ($K=5$ in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [-inf  -1.   0.   1.   3.]\n",
      "exp(x) =  [ 0.          0.36787944  1.          2.71828183 20.08553692]\n",
      "denominator (sum of exp(x_j), j=1,...,K) =  24.171698192818155\n",
      "softmax(x) =  [0.         0.01521943 0.0413707  0.11245721 0.83095266]\n",
      "sum of softmax(x)_j, j=1,...,K =  1.0\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "x = np.array([-np.Inf, -1., 0., 1., 3.])\n",
    "keras.layers.Activation('softmax')(x)\n",
    "print(\"x = \", x)\n",
    "print(\"exp(x) = \", np.exp(x))\n",
    "print(\"denominator (sum of exp(x_j), j=1,...,K) = \", sum(np.exp(x)))\n",
    "print(\"softmax(x) = \", np.exp(x) / sum(np.exp(x)))\n",
    "print(\"sum of softmax(x)_j, j=1,...,K = \", sum(np.exp(x) / sum(np.exp(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the softmax function really makes the largest numbers stand out from the rest.\n",
    "\n",
    "Note also that $-\\infty$ results in 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Multi-head attention\n",
    "\n",
    "* When values are shared in each head and then are aggregated additively, each head still lcan learn different temporal patterns (from their own unique queries and keys), but with the same input values.\n",
    "    * In other words, they can be interpreted as an ensemble over the attention weights\n",
    "    * the paper doesn't mention this explicitly, but the ensemble is equally-weighted - maybe there is some performance to be gained by having some way to weight the different attention heads ðŸ¤”, such as having a linear layer combining them... will explore in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadAttention():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head:int,\n",
    "        d_model:int,\n",
    "        training:bool=True, # Whether the layer is being trained or used in inference\n",
    "        dropout:float=0.0 # Will be ignored if `training=False`\n",
    "    ):\n",
    "        self.n_head = n_head\n",
    "        self.d_k = self.d_v = d_k = d_v = d_model # // n_head - the original model divides by number of heads\n",
    "        self.training = training\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # using the same value layer facilitates interpretability\n",
    "        vs_layer = keras.layers.Dense(d_v, use_bias=False, name=\"Shared value\")\n",
    "\n",
    "        # creates list of queries, keys and values across heads\n",
    "        self.qs_layers = self._build_layers(d_k, n_head)\n",
    "        self.ks_layers = self._build_layers(d_k, n_head)\n",
    "        self.vs_layers = [vs_layer for _ in range(n_head)]\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.w_o = keras.layers.Dense(d_v, use_bias=False, name=\"W_v\") # W_v in Eqs. (14)-(16), output weight matrix to project internal state to the original TFT\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q, # Queries, tensor of shape (?, time, D_model)\n",
    "        k, # Keys, tensor of shape (?, time, D_model)\n",
    "        v, # Values, tensor of shape (?, time, D_model)\n",
    "        mask=None # Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)\n",
    "    ):\n",
    "        heads = []\n",
    "        attns = []\n",
    "        for i in range(self.n_head):\n",
    "            qs = self.qs_layers[i](q)\n",
    "            ks = self.ks_layers[i](q)\n",
    "            vs = self.vs_layers[i](v)\n",
    "           \n",
    "            head, attn = self.attention(qs, ks, vs, mask)\n",
    "            if self.training:\n",
    "                head = keras.layers.Dropout(self.dropout)(head)\n",
    "            heads.append(head)\n",
    "            attns.append(attn)\n",
    "\n",
    "        outputs = keras.ops.mean(heads, axis=0) if self.n_head > 1 else head # H_tilde\n",
    "        outputs = self.w_o(outputs)\n",
    "        if self.training:\n",
    "            outputs = keras.layers.Dropout(self.dropout)(outputs)\n",
    "\n",
    "        return outputs, attn\n",
    "\n",
    "    def _build_layers(self, d:int, n_head:int):\n",
    "            return [keras.layers.Dense(d) for _ in range(n_head)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "imha = InterpretableMultiHeadAttention(n_head=8, d_model=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 17])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grn.shape # B, T, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 5])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_decoder_mask(grn)\n",
    "mask.shape # B, T, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 16), dtype=float32, numpy=\n",
       " array([[ 1.6538911 , -1.5456989 , -0.15283048,  2.7386441 , -0.5256885 ,\n",
       "          0.71322477,  2.6480474 , -1.675452  , -1.5537498 , -1.8390272 ,\n",
       "          0.8307655 , -0.17164582,  1.3680466 ,  0.5958487 ,  2.9639983 ,\n",
       "          2.0415192 ],\n",
       "        [-0.01169664,  1.2011886 ,  0.8329091 ,  2.2552555 , -1.3456283 ,\n",
       "         -0.4282118 ,  0.5777761 ,  0.19704199, -0.7254917 , -0.58820635,\n",
       "         -1.2332759 , -0.4459725 , -0.6127497 ,  0.7004302 , -0.20625532,\n",
       "          0.64137644],\n",
       "        [-0.09603077,  0.3872428 , -0.8894279 ,  0.6738455 ,  0.2292835 ,\n",
       "         -0.16534019,  0.4301936 , -0.746166  , -0.5417243 ,  0.15263456,\n",
       "         -1.5832616 ,  0.12967631, -0.56263334,  0.46816152, -0.5202312 ,\n",
       "         -0.59023654]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[0.11542334, 0.22207421, 0.3132397 , 0.25249708, 0.09676565],\n",
       "         [0.        , 0.06326903, 0.50399   , 0.3407762 , 0.09196478],\n",
       "         [0.        , 0.        , 0.4328694 , 0.17137659, 0.395754  ],\n",
       "         [0.        , 0.        , 0.        , 0.1503429 , 0.8496571 ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 1.        ]],\n",
       " \n",
       "        [[0.24687238, 0.22935054, 0.0300589 , 0.36477134, 0.12894684],\n",
       "         [0.        , 0.04162426, 0.15346661, 0.46041766, 0.34449142],\n",
       "         [0.        , 0.        , 0.34159458, 0.13954158, 0.5188638 ],\n",
       "         [0.        , 0.        , 0.        , 0.96586853, 0.03413144],\n",
       "         [0.        , 0.        , 0.        , 0.        , 1.        ]],\n",
       " \n",
       "        [[0.06245377, 0.13695577, 0.46981427, 0.07770143, 0.25307474],\n",
       "         [0.        , 0.34181076, 0.0816209 , 0.0560144 , 0.52055395],\n",
       "         [0.        , 0.        , 0.20488834, 0.7746067 , 0.02050495],\n",
       "         [0.        , 0.        , 0.        , 0.70111185, 0.29888812],\n",
       "         [0.        , 0.        , 0.        , 0.        , 1.        ]]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imha(grn, grn, grn, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to build the TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalFusionTransformer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Data params\n",
    "        time_steps:int,\n",
    "        input_size:int,\n",
    "        output_size:int,\n",
    "        category_counts:int,\n",
    "        n_workers:int, # Number of multiprocessing workers\n",
    "\n",
    "        # TFT params\n",
    "        input_obs_loc,\n",
    "        static_input_loc,\n",
    "        known_regular_input_idx,\n",
    "        known_categorical_input_idx,\n",
    "        column_definition,\n",
    "\n",
    "        # Network params\n",
    "        quantile:list=[0.1, 0.5, 0.9], # List of quantiles the model should forecast\n",
    "        hidden_layer_size:int=30, # Size of hidden layer\n",
    "        dropout_ratio:float=0.0, # Dropout ratio (between 0.0, inclusive, and less than 1.0)\n",
    "        num_encoder_steps:int=4,\n",
    "        num_stacks:int=4,\n",
    "        num_heads:int=4,\n",
    "        \n",
    "        # Training params\n",
    "        max_gradient_norm:float=1.0, # \n",
    "        learning_rate:float=0.001,\n",
    "        minibatch_size:int=64,\n",
    "        num_epochs:int=100,\n",
    "        early_stopping_patience:int=5,\n",
    "        use_gpu:bool=True\n",
    "    ):\n",
    "        self.time_steps = time_steps,\n",
    "        self.input_size = input_size,\n",
    "        self.output_size = output_size, # Number of periods to be forecasted\n",
    "        self.category_counts = category_counts,\n",
    "        self.n_workers = n_workers, # Number of multiprocessing workers\n",
    "        \n",
    "        self.input_obs_loc = input_obs_loc,\n",
    "        self.static_input_loc = static_input_loc,\n",
    "        self.known_regular_input_idx = known_regular_input_idx,\n",
    "        self.known_categorical_input_idx = known_categorical_input_idx,\n",
    "        self.column_definition = column_definition,\n",
    "\n",
    "        self.quantile = quantile, # List of quantiles the model should forecast\n",
    "        self.hidden_layer_size = hidden_layer_size, # Size of hidden layer\n",
    "        self.dropout_ratio = dropout_ratio, # Dropout ratio (between 0.0, inclusive, and less than 1.0)\n",
    "        self.num_encoder_steps = num_encoder_steps,\n",
    "        self.num_stacks = num_stacks,\n",
    "        self.num_heads = num_heads,\n",
    "        \n",
    "        self.max_gradient_norm = max_gradient_norm,\n",
    "        self.learning_rate = learning_rate,\n",
    "        self.minibatch_size = minibatch_size,\n",
    "        self.num_epochs = num_epochs,\n",
    "        self.early_stopping_patience = early_stopping_patience,\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def __static_combine_and_mask(\n",
    "        self, \n",
    "        embedding # Transformed static inputs\n",
    "    ):\n",
    "        # Return the tensor output for the variable selection network\n",
    "        # In the paper, that would be the bottom right square in Fig.2\n",
    "\n",
    "        # Add temporal features\n",
    "        _, num_static, _ = embedding.get_shape().as_list() # (embeddings are $\\xi_t^(1, \\dots, \\m_{\\chi})$)\n",
    "        flattened = keras.layers.Flatten()(embedding) # $\\Xi_t$\n",
    "\n",
    "        # Nonlinear transformation with the GRN\n",
    "        mlp_outputs = gated_residual_network(\n",
    "            x=flattened, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=num_static, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "            additional_context=None, # Additional context vector to use if relevant\n",
    "        ) \n",
    "        sparse_weights = keras.layers.Activation('softmax')(mlp_outputs)\n",
    "        sparse_weights = keras.ops.expand_dims(sparse_weights, axis=-1) # $\\upsilon_{\\chi t}$\n",
    "        # it's the sparse weights above that determine by how much the variable will be influencing the model\n",
    "\n",
    "        transformed_embeddings = []\n",
    "        for i in range(num_static):\n",
    "            transformed_embeddings.append(gated_residual_network(\n",
    "                x=embedding[:, i:i+1, :], # Network inputs\n",
    "                hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "                output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "                dropout_rate=self.dropout_rate, # Dropout rate\n",
    "                use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "            ))\n",
    "        transformed_embedding = keras.ops.concatenate(transformed_embeddings, axis=1) # $\\tilde{\\xi_t^(1, \\dots, \\m_{\\chi})}$\n",
    "\n",
    "        combined = keras.layers.Multiply()(\n",
    "            [sparse_weights, transformed_embedding]\n",
    "        )\n",
    "        \n",
    "        static_vec = keras.ops.sum(combined, axis=1)\n",
    "\n",
    "        return static_vec, sparse_weights\n",
    "\n",
    "    def _build_base_graph(self):\n",
    "        # Build the graph, defining the layers of the TFT\n",
    "        \n",
    "        all_inputs = keras.layers.Input(\n",
    "            shape=(self.time_steps, self.input_size)\n",
    "        )\n",
    "        unknown_inputs, known_combined_layer, past_inputs, static_inputs \\\n",
    "            = self.get_tft_embeddings(all_inputs)\n",
    "\n",
    "        # first we isolate the known future inputs and observed past inputs\n",
    "        if unknown_inputs is not None:\n",
    "            historical_inputs = keras.ops.concatenate([\n",
    "                unknown_inputs[:, :self.num_encoder_steps, :],\n",
    "                known_combined_layer[:, :self.num_encoder_steps, :],\n",
    "                past_inputs[:, :self.num_encoder_steps, :]\n",
    "            ], axis=1)\n",
    "        else:\n",
    "            historical_inputs = keras.ops.concatenate([\n",
    "                known_combined_layer[:, :self.num_encoder_steps, :],\n",
    "                past_inputs[:, :self.num_encoder_steps, :]\n",
    "            ])\n",
    "        \n",
    "        # and then we isolate the known future inputs\n",
    "        future_inputs = known_combined_layer[:, :self.num_encoder_steps, :]\n",
    "\n",
    "        # static vars\n",
    "        static_encoder, static_weights = self.__static_combine_and_mask(static_inputs)\n",
    "\n",
    "        # static enrichment\n",
    "        static_context_variable_selection = gated_residual_network(\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all timesteps?\n",
    "        )\n",
    "        static_context_enrichment\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build model and define training losses\n",
    "\n",
    "        transformer_layer, all_inputs, self._attention_components = self._build_base_graph()\n",
    "        outputs = keras.layers.TimeDistributed(\n",
    "            keras.layers.Dense(self.output_size * len(self.quantiles))\n",
    "        )(transformer_layer[Ellipsis, self.num_encoder_steps:, :])\n",
    "        model = keras.Model(inputs=all_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb Cell 65\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tft \u001b[39m=\u001b[39m TemporalFusionTransformer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     time_steps\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     input_size\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     output_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     category_counts\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     n_workers\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m# Number of multiprocessing workers\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# TFT params\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     input_obs_loc\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     static_input_loc\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     known_regular_input_idx\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     known_categorical_input_idx\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     column_definition\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb Cell 65\u001b[0m in \u001b[0;36mTemporalFusionTransformer.__init__\u001b[0;34m(self, time_steps, input_size, output_size, category_counts, n_workers, input_obs_loc, static_input_loc, known_regular_input_idx, known_categorical_input_idx, column_definition, quantile, hidden_layer_size, dropout_ratio, num_encoder_steps, num_stacks, num_heads, max_gradient_norm, learning_rate, minibatch_size, num_epochs, early_stopping_patience, use_gpu)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mearly_stopping_patience \u001b[39m=\u001b[39m early_stopping_patience,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_gpu \u001b[39m=\u001b[39m use_gpu\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_model()\n",
      "\u001b[1;32m/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb Cell 65\u001b[0m in \u001b[0;36mTemporalFusionTransformer.build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39m# Build model and define training losses\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     transformer_layer, all_inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attention_components \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_base_graph()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     outputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mTimeDistributed(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantiles))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     )(transformer_layer[\u001b[39mEllipsis\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_encoder_steps:, :])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/douglasaraujo/Coding/TemporalFusionTransformers/temporal_fusion_transformers.ipynb#Y140sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39mall_inputs, outputs\u001b[39m=\u001b[39moutputs)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer(\n",
    "    time_steps=12,\n",
    "    input_size=20,\n",
    "    output_size=4,\n",
    "    category_counts=5,\n",
    "    n_workers=2, # Number of multiprocessing workers\n",
    "\n",
    "    # TFT params\n",
    "    input_obs_loc=24,\n",
    "    static_input_loc=24,\n",
    "    known_regular_input_idx=24,\n",
    "    known_categorical_input_idx=24,\n",
    "    column_definition=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# References {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv_gingado')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977c2d9435ad3a481cf1bbece8d5ecb19e078de55648a0a0bad32b79c2e18340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
