{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Experimenting with TFT\"\n",
    "author: Douglas Araujo\n",
    "format: \n",
    "    html:\n",
    "        toc: true\n",
    "        toc-location: right\n",
    "        toc-depth: 4\n",
    "        number-sections: true\n",
    "        code-fold: true\n",
    "        code-tools: true\n",
    "        embed-resources: true\n",
    "bibliography: ref.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the temporal fusion transformers [@lim2021temporal] architecture, and ports it over to keras 3 while making some punctual improvements, including making the notation closer to the paper math.\n",
    "\n",
    "The original repository is: https://github.com/google-research/google-research/tree/master/tft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import keras_core as keras\n",
    "from keras_core import layers\n",
    "from fastcore import docments\n",
    "from nbdev.showdoc import show_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main characteristics of TFT that make it interesting for nowcasting or forecasting purposes are:\n",
    "\n",
    "- **multi-horizon forecasting**: the ability to output, at each point in time $t$, a sequence of forecasts for $t+h, h > 1$\n",
    "- **quantile prediction**: each forecast is accompanied by a quantile band that communicates the amount of uncertainty around a prediction\n",
    "- **flexible use of different types of inputs**: static inputs (akin to fixed effects), historical input and known future input (eg, important holidays, years that are known to have major sports events such as Olympic games, etc)\n",
    "- **interpretability**: the model learns to select variables from the space of all input variables to retain only those that are globally meaningful, to assign attention to different parts of the time series, and to identify events of significance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "\n",
    "* unique entities: $i \\in (1, \\dots\\, I)$\n",
    "* time periods $t \\in [0, T_i]$\n",
    "  * $k \\geq 1$ lags\n",
    "  * $h \\geq 1$ forecasting period\n",
    "* set of entity-level static covariates: $s_i \\in \\mathbf{R}^{m_s}$\n",
    "* set of temporal inputs: $\\chi_{i, t} \\in \\mathbf{R}^{m_\\chi}$\n",
    "  * $\\chi_{i,t} = [z_{i,t}, x_{i,t}]$\n",
    "    * $z_{i,t} \\in \\mathbf{R}^{m_z}$ are observed inputs\n",
    "    * $x_{i,t} \\in \\mathbf{R}^{m_z}$ are a priori known inputs (eg, years that have major sports events)\n",
    "    * $m_\\chi = m_z + m_x$\n",
    "* target scalars: $y_{i,t}$\n",
    "  * $\\hat{y}_{i,t,q} = f_q(y_{i,t-k:t}, z_{i,t-k:t}, x_{i,t-k:t+h}, s_i)$\n",
    "* hidden unit size (common across all the TFT architecture): $d_{\\text{model}}$\n",
    "* transformed input of $j$-th variable at time $t$: $\\xi_t^{(j)} \\in \\mathbf{R}^{d_{\\text{model}}}$\n",
    "  * $\\Xi_t = [\\xi_t^{(1)}, \\dots, \\xi_t^{(m_\\chi)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **time distributed**: \n",
    "  * applies same layer to each of the time steps in the data\n",
    "    * in other words, a layer with the exact same weights\n",
    "  * indices:\n",
    "    * index 0: batch\n",
    "    * index 1: time\n",
    "    * indices 2...: data\n",
    "  * More info: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer\n",
    "\n",
    "> dedicated implementation to better control use of time distribution on vanilla linear layer\n",
    "\n",
    "$$\n",
    "\\mathbb{Y} = \\phi(\\mathbf{W} x + \\mathbf{b}),\n",
    "$$ {#eq-dense}\n",
    "\n",
    "where $x$ is the input to `linear_layer()(x)`, $\\mathbb{Y}$ is the output of `linear_layer()(x)`, $\\phi$ is an activation function (or no activation function is `activation` is `None`), $\\mathbf{W} \\in \\mathbf{R}^{(d_{\\text{size}} \\times d_{\\text{inputs}})}$ is a matrix of weights and $\\mathbf{b} \\in \\mathbf{R}^{d_{size}}$ is a vector of biases. Importantly, $\\mathbf{W}$ and $\\mathbf{b}$ are indexed with $_{\\omega}$ to denote weight-sharing when the layer is time-distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(size:int, # Output size\n",
    "                 activation:str|callable|None=None, # Activation function\n",
    "                 use_time_distributed:bool=False, # Apply the layer across all time steps?\n",
    "                 use_bias:bool=True # Include bias in the layer?\n",
    ")->layers.Dense: # Dense layer\n",
    "    \"Linear layer.\"\n",
    "\n",
    "    linear = layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "    if use_time_distributed:\n",
    "        linear = layers.TimeDistributed(linear)\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### linear_layer\n",
       "\n",
       "\n",
       "\n",
       "Linear layer."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### linear_layer\n",
       "\n",
       "\n",
       "\n",
       "Linear layer."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "show_doc(linear_layer, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 2\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "layer_size = 8\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.ones(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# dense layer\n",
    "linear_td_true = linear_layer(size=layer_size, use_time_distributed=True)\n",
    "linear_td_false = linear_layer(size=layer_size, use_time_distributed=False)\n",
    "\n",
    "# output dimensions: batches / timesteps / layer size\n",
    "assert linear_td_true(x).shape == [batch_size, n_timesteps, layer_size]\n",
    "assert linear_td_false(x).shape == [batch_size, n_timesteps, layer_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the time-distributed linear layer results in the same weights being applied to each time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 8), dtype=float32, numpy=\n",
       "array([[[-0.27810854,  1.139082  , -1.7507869 ,  2.0346196 ,\n",
       "          1.0295256 ,  1.7452098 ,  0.4395818 ,  0.54309237],\n",
       "        [-0.27810854,  1.139082  , -1.7507869 ,  2.0346196 ,\n",
       "          1.0295256 ,  1.7452098 ,  0.4395818 ,  0.54309237],\n",
       "        [-0.27810854,  1.139082  , -1.7507869 ,  2.0346196 ,\n",
       "          1.0295256 ,  1.7452098 ,  0.4395818 ,  0.54309237],\n",
       "        [-0.27810854,  1.139082  , -1.7507869 ,  2.0346196 ,\n",
       "          1.0295256 ,  1.7452098 ,  0.4395818 ,  0.54309237],\n",
       "        [-0.27810854,  1.139082  , -1.7507869 ,  2.0346196 ,\n",
       "          1.0295256 ,  1.7452098 ,  0.4395818 ,  0.54309237]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "x = np.ones((1, n_timesteps, n_features))\n",
    "timesteps_equal = []\n",
    "for i in range(n_timesteps-1):\n",
    "    timesteps_equal.append((np.array_equal(linear_td_true(x)[0,0,:], linear_td_true(x)[0,i+1,:])))\n",
    "\n",
    "assert np.all(timesteps_equal)\n",
    "\n",
    "linear_td_true(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip connection\n",
    "\n",
    "> Adds inputs to layer and then implements layer normalisation\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(a + b),\n",
    "$$ {#eq-skip}\n",
    "\n",
    "for $a$ and $b$ tensors of the same dimension and $\\text{LayerNorm}(\\cdot)$ being the layer normalisation (@ba2016layer), ie subtracting $\\mu^l$ and dividing by $\\sigma^l$ defined as:\n",
    "\n",
    "$$\n",
    "\\mu^l = \\frac{1}{H} \\sum_{i=1}^H n_i^l \\quad \\sigma^l = \\sqrt{\\frac{1}{H} \\sum_{i=1}^H (n_i^l - \\mu^l)^2},\n",
    "$$ {#eq-layernorm}\n",
    "\n",
    "with $H$ denoting the number of $n$ hidden units in a layer $l$.\n",
    "\n",
    "* Adding a layer's inputs to its outputs is also called \"skip connection\"\n",
    "* The layer is then normalised [@ba2016layer] to avoid having the numbers grow too big, which is detrimental for gradient transmission\n",
    "  * Layer normalisation uses the same computation both during training and inference times, and is particularly suitable for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_norm(\n",
    "    x_list # List of input tensors (of the same dimension) for skip connection\n",
    "    ):\n",
    "    \"Adds tensors with same dimensions and then normalises layer\"\n",
    "    tmp = layers.Add()(x_list)\n",
    "    return layers.LayerNormalization()(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### add_and_norm\n",
       "\n",
       ">      add_and_norm (x_list)\n",
       "\n",
       "Adds tensors with same dimensions and then normalises layer\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| x_list | List of input tensors (of the same dimension) for skip connection |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### add_and_norm\n",
       "\n",
       ">      add_and_norm (x_list)\n",
       "\n",
       "Adds tensors with same dimensions and then normalises layer\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| x_list | List of input tensors (of the same dimension) for skip connection |"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "show_doc(add_and_norm, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x1 = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "x2 = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# output dimensions: batches / timesteps / features\n",
    "x1x2 = add_and_norm(x_list=[x1, x2])\n",
    "assert x1.shape == x1x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean values of the normalised layer should be around 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of x1 units at each batch X time step:  [[ 0.0891     -0.00987941 -0.10854553 -0.00655235  0.07156264]\n",
      " [-0.05450076 -0.06417484  0.07080917 -0.13651035  0.08508652]\n",
      " [ 0.0993336  -0.09902981 -0.02505386  0.04154724 -0.11811789]] \n",
      "\n",
      "mean of x2 units at each batch X time step:  [[ 0.00124595 -0.08214826  0.01206193 -0.01212819  0.11724552]\n",
      " [ 0.03407577 -0.17638668  0.11328357 -0.20265001  0.00658767]\n",
      " [-0.00163865  0.04814776 -0.04867956 -0.08390731 -0.02609097]] \n",
      "\n",
      "mean of sum of x1 and x2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " array([[-1.22189521e-08,  1.10268594e-08,  2.08616258e-09,\n",
       "          1.81794171e-08,  4.76837148e-09],\n",
       "        [ 9.53674295e-09,  4.61936001e-09, -3.79979603e-09,\n",
       "          1.07288365e-08,  1.78813941e-08],\n",
       "        [-2.02655794e-08, -2.20537189e-08,  7.00354574e-09,\n",
       "         -8.34465030e-09, -1.49011609e-10]], dtype=float32))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"mean of x1 units at each batch X time step: \", x1.mean(axis=-1), \"\\n\")\n",
    "print(\"mean of x2 units at each batch X time step: \", x2.mean(axis=-1), \"\\n\")\n",
    "print(\"mean of sum of x1 and x2:\"), x1x2.numpy().mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation (for the normalised output it should be around 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std of x1 units at each batch X time step:  [[0.94563709 0.96309248 0.97245555 1.01093958 0.8727528 ]\n",
      " [1.03880269 1.08403902 0.94582936 0.96116156 1.036372  ]\n",
      " [0.95825912 1.02954164 0.98003212 1.01333929 0.95075193]] \n",
      "\n",
      "std of x2 units at each batch X time step:  [[0.92853231 0.89098436 0.92638462 1.168471   1.06304328]\n",
      " [0.99139127 0.89403001 1.06474063 1.01128404 1.0370899 ]\n",
      " [1.16739561 0.96358718 0.97307712 1.02691728 0.88659434]] \n",
      "\n",
      "std of normalised sum of x1 and x2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " array([[0.9997464 , 0.99971324, 0.9997391 , 0.99975777, 0.99971575],\n",
       "        [0.99971294, 0.99970776, 0.9997577 , 0.99974173, 0.9997703 ],\n",
       "        [0.99977344, 0.99974054, 0.9997541 , 0.99974614, 0.9996765 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"std of x1 units at each batch X time step: \", x1.std(axis=-1), \"\\n\")\n",
    "print(\"std of x2 units at each batch X time step: \", x2.std(axis=-1), \"\\n\")\n",
    "print(\"std of normalised sum of x1 and x2:\"), x1x2.numpy().std(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated linear unit (GLU)\n",
    "\n",
    "> Linear layer that learns how much to gate vs let pass through\n",
    "\n",
    "Using input $\\gamma \\in \\mathbb{R}^{d_{\\text{model}}}$ and the subscript $\\omega$ to index weights, \n",
    "\n",
    "$$\n",
    "\\text{GLU}_{\\omega}(\\gamma) = \\sigma(W_{4, \\omega} \\gamma + b_{4, \\omega}) \\odot (W_{5, \\omega} \\gamma + b_{5, \\omega}).\n",
    "$$ {#eq-GLU}\n",
    "\n",
    "* Introduced by @dauphin2017language\n",
    "* The intuition is to train two versions of @eq-dense in the same data, but one of them having a sigmoid activation (which outputs values between zero and one), then multiplying each hidden unit\n",
    "* The result could be zero or very close to zero through the Hadamard multipliciation, which in practice means that the network would not be affected by that data (ie, it would be gated out)\n",
    "  * The first term (with the sigmoid) is the gate that determines what percentage of the linear layer passes through\n",
    "* *\"GLUs reduce the vanishing gradient problem for deep architectures by providing a linear path for gradients while retaining non-linear capabilities\"*\n",
    "* *\"provide flexibility to suppress any parts of the architecture that are not required for a given dataset\"*\n",
    "* The GLU is part of @sec-GRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gating_layer(\n",
    "    x, # Input tensors (batch first)\n",
    "    hidden_layer_size:int, # Dimension of the GLU\n",
    "    dropout_rate:float|None=None, # Dropout rate\n",
    "    use_time_distributed:bool=True, # Apply the GLU across all time steps?\n",
    "    activation:str|callable=None # Activation function\n",
    ")->(keras.KerasTensor, keras.KerasTensor): # $\\text{GLU}(\\gamma)$, $\\sigma(W \\gamma + b)$, both with dimension (batch_size, num_time_steps, hidden_layer_size)\n",
    "    \"Gated Linear Unit (GLU) layer\"\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    activation_layer = linear_layer(\n",
    "        size=hidden_layer_size,\n",
    "        activation=activation,\n",
    "        use_time_distributed=use_time_distributed\n",
    "    )(x)\n",
    "    \n",
    "    gate_layer = linear_layer(\n",
    "        size=hidden_layer_size,\n",
    "        activation='sigmoid',\n",
    "        use_time_distributed=use_time_distributed\n",
    "    )(x)\n",
    "\n",
    "    return layers.Multiply()([activation_layer, gate_layer]), gate_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### apply_gating_layer\n",
       "\n"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### apply_gating_layer\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "\n",
    "show_doc(apply_gating_layer, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "hidden_layer_size = 16\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# output dimensions: batches / timesteps / hidden_layer_size\n",
    "assert apply_gating_layer(x=x, hidden_layer_size=hidden_layer_size)[0].shape == [batch_size, n_timesteps, hidden_layer_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TensorShape([3, 5, 16]), TensorShape([3, 5, 16])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "[i.shape for i in apply_gating_layer(x=x, hidden_layer_size=hidden_layer_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated residual network (GRN) {#sec-GRN}\n",
    "\n",
    "$$\n",
    "\\text{GRN}_{\\omega}(a, c) = \\text{LayerNorm}(a + \\text{GLU}_{\\omega}(W_{1, \\omega} \\text{ELU}(W_{2, \\omega} a + b_{2, \\omega} + W_{3, \\omega} c) + b_{1, w}))\n",
    "$$ {#eq-GRN}\n",
    "\n",
    "* Breaking down $\\text{GRN}_{\\omega}(a, c)$:\n",
    "    * *1st step*: $\\eta_{2} = \\text{ELU}(W_{2, \\omega} a + b_{2, \\omega} + W_{3, \\omega} c)$ (where the additional context $c$ might be zero) as in @eq-dense but adapted for the added context if any and with $\\text{ELU}(\\cdot)$ as the activation function,\n",
    "    * *2nd step*: $\\eta_{1} = W_{1, \\omega} \\eta_{2} + b_{1, w}$ as in @eq-dense,\n",
    "    * *3rd step*: $\\text{LayerNorm}(a + \\text{GLU}_{\\omega}(\\eta_{1}))$ as in @eq-skip and @eq-GLU\n",
    "* $\\text{ELU}(\\cdot)$ is the Exponential Linear Unit activation function (@clevert2015fast)\n",
    "    * Unlike ReLUs, ELUs allow for negative values, which pushes unit activations closer to zero at a lower computation complexity, and producing more accurate results\n",
    "* The GRN is a key building block of the TFT\n",
    "    * Helps keep information only from relevant input variables\n",
    "    * Also keeps the model as simple as possible by only applying non-linearities when relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_residual_network(\n",
    "    x, # Network inputs\n",
    "    hidden_layer_size:int, # Dimension of the GRN\n",
    "    output_size:int|None=None, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "    dropout_rate:float|None=None, # Dropout rate\n",
    "    use_time_distributed:bool=True, # Apply the GRN across all time steps?\n",
    "    additional_context=None, # Additional context vector to use if relevant\n",
    "    return_gate:bool=False #Whether to return GRN gate for diagnostic purposes\n",
    "):\n",
    "    \"Applies the gated residual network (GRN) as defined in the paper\"\n",
    "    \n",
    "    # Setup skip connection\n",
    "    if output_size is None:\n",
    "        output_size = hidden_layer_size\n",
    "        skip = x\n",
    "    else:\n",
    "        linear = keras.layers.Dense(output_size)\n",
    "        if use_time_distributed:\n",
    "            linear = keras.layers.TimeDistributed(linear)\n",
    "        skip = linear(x)\n",
    "\n",
    "    # 1st step: eta2\n",
    "    hidden = linear_layer(\n",
    "        size=hidden_layer_size, # W2\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=True # b2\n",
    "    )(x)\n",
    "\n",
    "    # \"For instances without a context vector, the GRN simply treates the context input as zero - ie, $c = 0$ in Eq. 4\"\n",
    "    if additional_context is not None: # if c is != 0...\n",
    "        hidden += linear_layer(\n",
    "            size=hidden_layer_size, # W3\n",
    "            activation=None,\n",
    "            use_time_distributed=use_time_distributed,\n",
    "            use_bias=False # no bias for additional context, since there already is bias from the \"main\" calculation of eta2\n",
    "        )(additional_context)\n",
    "\n",
    "    hidden = keras.layers.Activation('elu')(hidden)\n",
    "\n",
    "    # 2nd step: eta1\n",
    "    hidden = linear_layer(\n",
    "        size=hidden_layer_size, # W1\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=True # b1\n",
    "    )(hidden)\n",
    "\n",
    "    # 3rd step: concluding the GRN calculation\n",
    "    gating_layer, gate = apply_gating_layer(\n",
    "        x=hidden,\n",
    "        hidden_layer_size=output_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        activation=None\n",
    "    )\n",
    "\n",
    "    GRN = add_and_norm([skip, gating_layer])\n",
    "\n",
    "    if return_gate:\n",
    "        return GRN, gate\n",
    "    else:\n",
    "        return GRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### gated_residual_network\n",
       "\n",
       ">      gated_residual_network (x, hidden_layer_size:int,\n",
       ">                              output_size:int|None=None,\n",
       ">                              dropout_rate:float|None=None,\n",
       ">                              use_time_distributed:bool=True,\n",
       ">                              additional_context=None, return_gate:bool=False)\n",
       "\n",
       "Applies the gated residual network (GRN) as defined in the paper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Network inputs |\n",
       "| hidden_layer_size | int |  | Dimension of the GRN |\n",
       "| output_size | int \\| None | None | Size of output layer (if None, same as `hidden_layer_size`) |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GRN across all time steps? |\n",
       "| additional_context | NoneType | None | Additional context vector to use if relevant |\n",
       "| return_gate | bool | False | Whether to return GRN gate for diagnostic purposes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### gated_residual_network\n",
       "\n",
       ">      gated_residual_network (x, hidden_layer_size:int,\n",
       ">                              output_size:int|None=None,\n",
       ">                              dropout_rate:float|None=None,\n",
       ">                              use_time_distributed:bool=True,\n",
       ">                              additional_context=None, return_gate:bool=False)\n",
       "\n",
       "Applies the gated residual network (GRN) as defined in the paper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Network inputs |\n",
       "| hidden_layer_size | int |  | Dimension of the GRN |\n",
       "| output_size | int \\| None | None | Size of output layer (if None, same as `hidden_layer_size`) |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GRN across all time steps? |\n",
       "| additional_context | NoneType | None | Additional context vector to use if relevant |\n",
       "| return_gate | bool | False | Whether to return GRN gate for diagnostic purposes |"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "\n",
    "show_doc(gated_residual_network, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "hidden_layer_size = 16\n",
    "output_size = 17\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "grn, gate = gated_residual_network(\n",
    "    x=x,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=0,\n",
    "    use_time_distributed=True,\n",
    "    additional_context=None,\n",
    "    return_gate=True\n",
    ")\n",
    "\n",
    "# output dimensions: batches / timesteps / hidden_layer_size\n",
    "assert grn.shape == [batch_size, n_timesteps, output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 17), dtype=float32, numpy=\n",
       "array([[0.46408975, 0.51650125, 0.3576629 , 0.32816413, 0.28808403,\n",
       "        0.25894374, 0.80485815, 0.6035425 , 0.18347855, 0.20668997,\n",
       "        0.40690467, 0.77098   , 0.5859963 , 0.77419156, 0.5004397 ,\n",
       "        0.6925783 , 0.8398583 ],\n",
       "       [0.5860845 , 0.4493843 , 0.35765216, 0.4549006 , 0.4134464 ,\n",
       "        0.4788437 , 0.7112705 , 0.42405283, 0.8090347 , 0.63934696,\n",
       "        0.12561484, 0.19756034, 0.29987946, 0.82742363, 0.40238598,\n",
       "        0.19599892, 0.2811166 ],\n",
       "       [0.49277538, 0.47769853, 0.51799566, 0.5151028 , 0.49813136,\n",
       "        0.37231475, 0.45661026, 0.51294667, 0.3605565 , 0.3776353 ,\n",
       "        0.6286485 , 0.7145511 , 0.4172555 , 0.62817055, 0.37960175,\n",
       "        0.67153347, 0.7933012 ],\n",
       "       [0.6383022 , 0.67520565, 0.36656207, 0.6027976 , 0.52373886,\n",
       "        0.41002965, 0.12543976, 0.6682144 , 0.41265413, 0.43105012,\n",
       "        0.5055955 , 0.6043989 , 0.4804591 , 0.7942763 , 0.5494791 ,\n",
       "        0.27742058, 0.6879719 ],\n",
       "       [0.55686593, 0.76892126, 0.2754986 , 0.83925945, 0.4541192 ,\n",
       "        0.6133421 , 0.27932784, 0.65525585, 0.6069252 , 0.7748477 ,\n",
       "        0.24409628, 0.35641775, 0.38788834, 0.75952476, 0.5524691 ,\n",
       "        0.20539647, 0.3164921 ]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "gate[0] # first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable selection networks\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{m_{\\chi}} \\upsilon_{\\chi_t}^{(j)} \\tilde{\\xi}_t^{(j)},\n",
    "$$ {#eq-VSN}\n",
    "\n",
    "with $j$ indexing the input variable, $\\upsilon_{\\chi_t}^{(j)}$ standing for variable $j$'s selection weight, $m$ being the number of features and $\\tilde{\\xi}_t^{(j)}$ defined as:\n",
    "\n",
    "$$\n",
    "\\tilde{\\xi}_t^{(j)} = \\text{GRN}(\\xi_t^{(j)}).\n",
    "$$ {#eq-embed}\n",
    "\n",
    "* In the paper, they are represented in the bottom right of Fig. 2\n",
    "* Note there are separate variable selection networks for different input groups:\n",
    "  * `static_variable_selection`\n",
    "    * does not have static context as input, as it already contains static information\n",
    "  * `temporal_variable_selection`\n",
    "    * used for both historical and known future inputs\n",
    "    * includes static contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_variable_selection( \n",
    "    embedding, # Embedded static inputs, $\\xi_t$\n",
    "    hidden_layer_size, # Dimension of the GRN\n",
    "    dropout_rate # Dropout rate\n",
    ")->(keras.KerasTensor, keras.KerasTensor): # $\\tilde{\\xi}_t$ with dimension (batch_size, ), $\\upsilon_{\\chi t}$ with dimension (batch_size, num_static_vars, 1)\n",
    "    \"Filter contribution of different static variables\"\n",
    "\n",
    "    # Add temporal features\n",
    "    _, num_static, _ = embedding.get_shape().as_list() # (embeddings are $\\xi_t^(1, \\dots, \\m_{\\chi})$)\n",
    "    flattened = layers.Flatten()(embedding) # $\\Xi_t$, with dimensions (batch_size, num_entities)\n",
    "\n",
    "    # Nonlinear transformation with the GRN\n",
    "    mlp_outputs = gated_residual_network(\n",
    "        x=flattened, # Network inputs\n",
    "        hidden_layer_size=hidden_layer_size, # Dimension of the GRN\n",
    "        output_size=num_static, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "        dropout_rate=dropout_rate, # Dropout rate\n",
    "        use_time_distributed=False, # Apply the GRN across all time steps?\n",
    "        additional_context=None, # Additional context vector to use if relevant\n",
    "    ) \n",
    "    sparse_weights = layers.Activation('softmax')(mlp_outputs)\n",
    "    sparse_weights = keras.ops.expand_dims(sparse_weights, axis=-1) # $\\upsilon_{\\chi t}$\n",
    "    # it's the sparse weights above that determine how much each variable will be influencing the model\n",
    "\n",
    "    transformed_embeddings = []\n",
    "    for i in range(num_static):\n",
    "        transformed_embeddings.append(gated_residual_network(\n",
    "            x=embedding[:, i:i+1, :], # Selects instances of the same static variable across all batches and individuals\n",
    "            hidden_layer_size=hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Does not make sense to apply the GRN across all time steps because static variables do not have a time dimension\n",
    "        ))\n",
    "    transformed_embedding = keras.ops.concatenate(transformed_embeddings, axis=1) # $\\tilde{\\xi_t^(1, \\dots, \\m_{\\chi})}$\n",
    "\n",
    "    combined = layers.Multiply()(\n",
    "        [sparse_weights, transformed_embedding]\n",
    "    )\n",
    "    \n",
    "    static_vec = keras.ops.sum(combined, axis=1)\n",
    "\n",
    "    return static_vec, sparse_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### static_variable_selection\n",
       "\n",
       ">      static_variable_selection (embedding, hidden_layer_size, dropout_rate)\n",
       "\n",
       "Filter contribution of different static variables\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| embedding |  | Embedded static inputs, $\\xi_t$ |\n",
       "| hidden_layer_size |  | Dimension of the GRN |\n",
       "| dropout_rate |  | Dropout rate |\n",
       "| **Returns** | **(keras.KerasTensor, keras.KerasTensor)** | **$\\tilde{\\xi}_t$ with dimension (batch_size, ), $\\upsilon_{\\chi t}$ with dimension (batch_size, num_static_vars, 1)** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### static_variable_selection\n",
       "\n",
       ">      static_variable_selection (embedding, hidden_layer_size, dropout_rate)\n",
       "\n",
       "Filter contribution of different static variables\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| embedding |  | Embedded static inputs, $\\xi_t$ |\n",
       "| hidden_layer_size |  | Dimension of the GRN |\n",
       "| dropout_rate |  | Dropout rate |\n",
       "| **Returns** | **(keras.KerasTensor, keras.KerasTensor)** | **$\\tilde{\\xi}_t$ with dimension (batch_size, ), $\\upsilon_{\\chi t}$ with dimension (batch_size, num_static_vars, 1)** |"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "\n",
    "show_doc(static_variable_selection, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_variable_selection(#__lstm_combine_and_mask(\n",
    "        embedding, # Embedded temporal inputs, $\\xi_t^(v), v \\in (1, \\dots, m_\\chi)$\n",
    "        context, # Static context variable selection, $c_s$\n",
    "        hidden_layer_size, # Dimension of the GRN, $d_{\\text{model}}$\n",
    "        dropout_rate # Dropout rate\n",
    "    )->(keras.KerasTensor, keras.KerasTensor, ): # $\\tilde{\\xi}_t$ with dimension (batch_size, ), $\\upsilon_{\\chi t}$ with dimension (batch_size, num_static_vars, 1):\n",
    "        \"Filter contribution of different temporal variables\"\n",
    "\n",
    "        # Add temporal features\n",
    "        _, time_steps, embedding_dim, num_inputs = embedding.get_shape().as_list()\n",
    "\n",
    "        flattened = keras.ops.reshape(\n",
    "            embedding,\n",
    "            [1, time_steps, embedding_dim * num_inputs]\n",
    "        )\n",
    "        expanded_static_context_c_s = keras.ops.expand_dims(\n",
    "            context,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Variable selection weights \\upsilon\n",
    "        mlp_outputs, upsilon = gated_residual_network(\n",
    "            x=flattened,\n",
    "            hidden_layer=hidden_layer_size,\n",
    "            output_size=num_inputs,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=True,\n",
    "            additional_context=expanded_static_context_c_s,\n",
    "            return_gate=True\n",
    "        )\n",
    "        sparse_weights = keras.layers.Activation('softmax')(mlp_outputs)\n",
    "        sparse_weights = keras.ops.expand_dims(sparse_weights, axis=2)\n",
    "\n",
    "        # Nonlinear processing and application of weights\n",
    "        transformed_embeddings = []\n",
    "        for i in range(num_inputs):\n",
    "            transformed_embeddings.append(\n",
    "                gated_residual_network(\n",
    "                    embedding[Ellipsis, i],\n",
    "                    hidde_layer=hidden_layer_size,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    use_time_distributed=True\n",
    "                )\n",
    "            )\n",
    "        transformed_embeddings = keras.ops.stack(transformed_embeddings, axis=-1)\n",
    "\n",
    "        combined = layers.Multiply()([\n",
    "            sparse_weights, transformed_embeddings\n",
    "        ])\n",
    "        temporal_ctx = keras.ops.sum(combined, axis=-1)\n",
    "\n",
    "        return temporal_ctx, sparse_weights, upsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "\n",
    "show_doc(temporal_variable_selection, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_static_vars = 8\n",
    "n_temporal_vars = 11\n",
    "n_indiv = 7\n",
    "hidden_layer_size = 16\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "static_vars = np.random.randn(batch_size*n_static_vars*n_indiv).reshape([batch_size, n_static_vars, n_indiv]) \n",
    "\n",
    "# transformed variables (\\xi_t^{(1)}, \\xi_t^{(2)})\n",
    "xi1 = linear_layer(\n",
    "        size=hidden_layer_size,\n",
    "        activation=None,\n",
    "        use_time_distributed=True\n",
    "        )(static_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([3, 16]), TensorShape([3, 8, 1]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "static_selected_vars, static_selection_weights = static_variable_selection(xi1, hidden_layer_size=hidden_layer_size, dropout_rate=0.)\n",
    "\n",
    "static_selected_vars.shape, static_selection_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention components\n",
    "\n",
    "* Attention mechanisms use relationships between keys $K \\in \\mathbf{R}^{N \\times d_{attention}}$ and queries $Q \\in \\mathbf{R}^{N \\times d_{attention}}$ to scale a vector of values $V \\in \\mathbf{R}^{N \\times d_V}$: $\\text{Attention}(Q, K, V) = A(Q, K) V$\n",
    "    * $N$ is the number of timesteps going into the attention layer (the number of lags plus the number of periods to be forecasted)\n",
    "    * $A(\\cdot)$ is a normalisation function\n",
    "        * After @vaswani2017attention, the canonical choice for $A(\\cdot)$ is the scaled dot-product: $A(Q, K) = \\text{Softmax}(\\frac{Q K^{T}}{\\sqrt{d_{attention}}} )$\n",
    "    \n",
    "* The TFT uses a modified attention head to enhance the explainability of the model\n",
    "* Specifically, the transformer block (multi-head attention) is modified to:\n",
    "    * share values in each head, and\n",
    "    * employ additive aggregation of all heads\n",
    "* More formally, compare the interpretable multi-head attention (used in this paper) with the canonical multi-head attention:\n",
    "    * $\\text{InterpretableMultiHead}(Q, K, V) = \\tilde{H} W_{H}$, with:\n",
    "        * $\\begin{aligned}\\tilde{H} &= \\tilde{A}(Q, K) V W_V \\\\\n",
    "        &= \\{\\frac{1}{m_H} \\sum^{m_{H}}_{h=1} A(Q W^{(h)}_Q, K W^{(h)}_K) \\} V W_V \\\\\n",
    "        &= \\frac{1}{m_H} \\sum^{m_{H}}_{h=1} \\text{Attention}(Q W^{(h)}_Q, K W^{(h)}_K, V W_V)\n",
    "        \\end{aligned}$\n",
    "    * $\\text{MultiHead}(Q, K, V) = [H_1, \\dots, H_{m_H}] W_H$, with:\n",
    "        * $H_h = \\text{Attention}(Q W^{(h)}_Q, K W^{(h)}_K, V W_V^{(h)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder mask for self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_mask(\n",
    "    self_attention_inputs # Inputs to the self-attention layer\n",
    "):\n",
    "    \"Determines shape of decoder mask\"\n",
    "    len_s = keras.ops.shape(self_attention_inputs)[1] # length of inputs\n",
    "    bs = keras.ops.shape(self_attention_inputs)[0] # batch shape\n",
    "    mask = keras.ops.cumsum(keras.ops.eye(len_s), 1) #keras.backend.cumsum(np.eye(len_s, bs))\n",
    "\n",
    "    ### warning: I had to manually implement some batch-wise shape here \n",
    "    ### because the new keras `eye` function does not have a batch_size arg.\n",
    "    ### inspired by: https://github.com/tensorflow/tensorflow/blob/v2.14.0/tensorflow/python/ops/linalg_ops_impl.py#L30\n",
    "    ### <hack>\n",
    "    mask = keras.ops.expand_dims(mask, axis=0)    \n",
    "    mask = keras.ops.tile(mask, (bs, 1, 1))\n",
    "    ### </hack>\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### get_decoder_mask\n",
       "\n",
       ">      get_decoder_mask (self_attention_inputs)\n",
       "\n",
       "Determines shape of decoder mask\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| self_attention_inputs | Inputs to the self-attention layer |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### get_decoder_mask\n",
       "\n",
       ">      get_decoder_mask (self_attention_inputs)\n",
       "\n",
       "Determines shape of decoder mask\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| self_attention_inputs | Inputs to the self-attention layer |"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "\n",
    "show_doc(get_decoder_mask, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "dec = get_decoder_mask(grn)\n",
    "\n",
    "assert dec.shape == (batch_size, n_timesteps, n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it produces an upper-triangular matrix of ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "dec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product attention layer\n",
    "\n",
    "* This is the same as Eq. (1) of @vaswani2017attention \n",
    "    * except that in this case the dimension of the value vector is the same $d_{\\text{model}}$ as for the query and key vectors\n",
    "* As discussed in the paper, additive attention outperforms dot product attention for larger $d_{\\text{model}}$ values, so the attention is scaled back to smaller values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention():\n",
    "    def __init__(\n",
    "        self,\n",
    "        training:bool=True, # Whether the layer is being trained or used in inference\n",
    "        attention_dropout:float=0.0 # Will be ignored if `training=False`\n",
    "    ):\n",
    "        self.training = training\n",
    "        self.dropout = keras.layers.Dropout(rate=attention_dropout)\n",
    "        self.activation = keras.layers.Activation('softmax')\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q, # Queries, tensor of shape (?, time, D_model)\n",
    "        k, # Keys, tensor of shape (?, time, D_model)\n",
    "        v, # Values, tensor of shape (?, time, D_model)\n",
    "        mask # Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)\n",
    "    ):\n",
    "        # returns Tuple (layer outputs, attention weights)\n",
    "        scale = keras.ops.sqrt(keras.ops.cast(keras.ops.shape(k)[-1], dtype='float32'))\n",
    "        attention = keras.ops.einsum(\"bij,bjk->bik\", q, keras.ops.transpose(k, axes=(0, 2, 1))) / scale\n",
    "        if mask is not None:\n",
    "            mmask = keras.layers.Lambda(lambda x: (-1e9) * (1. - keras.ops.cast(x, 'float32')))(mask)\n",
    "            attention = keras.layers.Add()([attention, mmask])\n",
    "        attention = self.activation(attention)\n",
    "        if self.training:\n",
    "            attention = self.dropout(attention)\n",
    "        output = keras.ops.einsum(\"btt,btd->bt\", attention, v)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (training:bool=True,\n",
       ">                                 attention_dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| attention_dropout | float | 0.0 | Will be ignored if `training=False` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (training:bool=True,\n",
       ">                                 attention_dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| attention_dropout | float | 0.0 | Will be ignored if `training=False` |"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "\n",
    "show_doc(ScaledDotProductAttention, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage\n",
    "Below is an example of how the `ScaledDotProductAttention` layer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 13\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x_btf = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# using the same vector for q, k and v just to simplify\n",
    "q=keras.ops.cast(x_btf, 'float32')\n",
    "k=keras.ops.cast(x_btf, 'float32')\n",
    "v=keras.ops.cast(x_btf, 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing without masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[ 1.0461457 ,  1.0315393 , -0.34412605, -0.45953444,  2.8253996 ],\n",
       "        [-1.744076  ,  4.6008143 , -1.4990771 , -5.1597986 ,  3.4189878 ],\n",
       "        [-1.9091827 ,  0.29079467, -0.11268348,  0.05857382,  1.3043103 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[3.86870772e-01, 1.28998399e-01, 2.67757922e-01, 7.61292577e-02,\n",
       "          1.40243649e-01],\n",
       "         [3.64807099e-02, 7.74610400e-01, 3.52141671e-02, 1.08375907e-01,\n",
       "          4.53188010e-02],\n",
       "         [8.30628444e-03, 3.86280683e-03, 9.78443503e-01, 2.39209924e-03,\n",
       "          6.99525373e-03],\n",
       "         [1.49935763e-02, 7.54757524e-02, 1.51868779e-02, 8.91941488e-01,\n",
       "          2.40225298e-03],\n",
       "         [1.44322505e-02, 1.64911319e-02, 2.32054479e-02, 1.25520967e-03,\n",
       "          9.44615960e-01]],\n",
       " \n",
       "        [[8.43910575e-01, 3.76040526e-02, 2.52377056e-02, 5.36176935e-02,\n",
       "          3.96299511e-02],\n",
       "         [1.51728827e-03, 9.95497584e-01, 1.39222902e-04, 1.48208230e-04,\n",
       "          2.69766594e-03],\n",
       "         [6.10941164e-02, 8.35269596e-03, 8.58545899e-01, 6.57059327e-02,\n",
       "          6.30136719e-03],\n",
       "         [6.77335113e-02, 4.64017410e-03, 3.42886560e-02, 8.70826840e-01,\n",
       "          2.25108843e-02],\n",
       "         [1.85574573e-02, 3.13075855e-02, 1.21893396e-03, 8.34434014e-03,\n",
       "          9.40571666e-01]],\n",
       " \n",
       "        [[9.40345943e-01, 4.86090314e-03, 2.84322333e-02, 6.82147406e-03,\n",
       "          1.95394233e-02],\n",
       "         [1.00748539e-02, 7.55925953e-01, 5.01050875e-02, 1.14619330e-01,\n",
       "          6.92747459e-02],\n",
       "         [8.82948563e-02, 7.50731006e-02, 3.41262847e-01, 4.28601563e-01,\n",
       "          6.67676404e-02],\n",
       "         [2.18510861e-03, 1.77145787e-02, 4.42103632e-02, 9.30254161e-01,\n",
       "          5.63572627e-03],\n",
       "         [4.57677543e-02, 7.82890320e-02, 5.03604561e-02, 4.12100367e-02,\n",
       "          7.84372687e-01]]], dtype=float32)>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "output, attention = ScaledDotProductAttention()(q=q, k=k, v=v, mask=None)\n",
    "output, attention # both have shape (batch_size, n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and with masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[ 1.0461457 ,  1.0705955 , -0.3483654 , -0.51382303,  2.9910564 ],\n",
       "        [-1.744076  ,  4.607806  , -1.6109525 , -5.775865  ,  3.6350102 ],\n",
       "        [-1.9091827 ,  0.29375416, -0.134687  ,  0.06258623,  1.6628706 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[3.86870772e-01, 1.28998399e-01, 2.67757922e-01, 7.61292577e-02,\n",
       "          1.40243649e-01],\n",
       "         [0.00000000e+00, 8.03938687e-01, 3.65474448e-02, 1.12479225e-01,\n",
       "          4.70346585e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 9.90497112e-01, 2.42156792e-03,\n",
       "          7.08142901e-03],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.97313917e-01,\n",
       "          2.68605119e-03],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]],\n",
       " \n",
       "        [[8.43910575e-01, 3.76040526e-02, 2.52377056e-02, 5.36176935e-02,\n",
       "          3.96299511e-02],\n",
       "         [0.00000000e+00, 9.97010350e-01, 1.39434473e-04, 1.48433464e-04,\n",
       "          2.70176539e-03],\n",
       "         [0.00000000e+00, 0.00000000e+00, 9.22618806e-01, 7.06095397e-02,\n",
       "          6.77163526e-03],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.74801302e-01,\n",
       "          2.51986254e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]],\n",
       " \n",
       "        [[9.40345943e-01, 4.86090314e-03, 2.84322333e-02, 6.82147406e-03,\n",
       "          1.95394233e-02],\n",
       "         [0.00000000e+00, 7.63619244e-01, 5.06150238e-02, 1.15785845e-01,\n",
       "          6.99797794e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 4.07900751e-01, 5.12293994e-01,\n",
       "          7.98052549e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.93978262e-01,\n",
       "          6.02178369e-03],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]]], dtype=float32)>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "output, attention = ScaledDotProductAttention()(q=q, k=k, v=v, mask=get_decoder_mask(q))\n",
    "output, attention # both have shape (batch_size, n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "A small detour to illustrate the softmax function. \n",
    "\n",
    "The $i^{\\text{th}}$ element of $\\text{Softmax}(x)$, with $x \\in \\mathbf{R}^K$ is:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, see the values below for an input vector $x$ ($K=5$ in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [-inf  -1.   0.   1.   3.]\n",
      "exp(x) =  [ 0.          0.36787944  1.          2.71828183 20.08553692]\n",
      "denominator (sum of exp(x_j), j=1,...,K) =  24.171698192818155\n",
      "softmax(x) =  [0.         0.01521943 0.0413707  0.11245721 0.83095266]\n",
      "sum of softmax(x)_j, j=1,...,K =  1.0\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "x = np.array([-np.Inf, -1., 0., 1., 3.])\n",
    "keras.layers.Activation('softmax')(x)\n",
    "print(\"x = \", x)\n",
    "print(\"exp(x) = \", np.exp(x))\n",
    "print(\"denominator (sum of exp(x_j), j=1,...,K) = \", sum(np.exp(x)))\n",
    "print(\"softmax(x) = \", np.exp(x) / sum(np.exp(x)))\n",
    "print(\"sum of softmax(x)_j, j=1,...,K = \", sum(np.exp(x) / sum(np.exp(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the softmax function really makes the largest numbers stand out from the rest.\n",
    "\n",
    "Note also that $-\\infty$ results in 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretable Multi-head attention\n",
    "\n",
    "* When values are shared in each head and then are aggregated additively, each head still lcan learn different temporal patterns (from their own unique queries and keys), but with the same input values.\n",
    "    * In other words, they can be interpreted as an ensemble over the attention weights\n",
    "    * the paper doesn't mention this explicitly, but the ensemble is equally-weighted - maybe there is some performance to be gained by having some way to weight the different attention heads , such as having a linear layer combining them... will explore in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadAttention():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head:int,\n",
    "        d_model:int,\n",
    "        training:bool=True, # Whether the layer is being trained or used in inference\n",
    "        dropout:float=0.0 # Will be ignored if `training=False`\n",
    "    ):\n",
    "        self.n_head = n_head\n",
    "        self.d_k = self.d_v = d_k = d_v = d_model // n_head # the original model divides by number of heads\n",
    "        self.training = training\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # using the same value layer facilitates interpretability\n",
    "        vs_layer = keras.layers.Dense(d_v, use_bias=False, name=\"Shared value\")\n",
    "\n",
    "        # creates list of queries, keys and values across heads\n",
    "        self.qs_layers = self._build_layers(d_k, n_head)\n",
    "        self.ks_layers = self._build_layers(d_k, n_head)\n",
    "        self.vs_layers = [vs_layer for _ in range(n_head)]\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.w_o = keras.layers.Dense(d_v, use_bias=False, name=\"W_v\") # W_v in Eqs. (14)-(16), output weight matrix to project internal state to the original TFT\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q, # Queries, tensor of shape (?, time, D_model)\n",
    "        k, # Keys, tensor of shape (?, time, D_model)\n",
    "        v, # Values, tensor of shape (?, time, D_model)\n",
    "        mask=None # Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)\n",
    "    ):\n",
    "        heads = []\n",
    "        attns = []\n",
    "        for i in range(self.n_head):\n",
    "            qs = self.qs_layers[i](q)\n",
    "            ks = self.ks_layers[i](q)\n",
    "            vs = self.vs_layers[i](v)\n",
    "           \n",
    "            head, attn = self.attention(qs, ks, vs, mask)\n",
    "            if self.training:\n",
    "                head = keras.layers.Dropout(self.dropout)(head)\n",
    "            heads.append(head)\n",
    "            attns.append(attn)\n",
    "\n",
    "        outputs = keras.ops.mean(heads, axis=0) if self.n_head > 1 else head # H_tilde\n",
    "        outputs = self.w_o(outputs)\n",
    "        if self.training:\n",
    "            outputs = keras.layers.Dropout(self.dropout)(outputs)\n",
    "\n",
    "        return outputs, attn\n",
    "\n",
    "    def _build_layers(self, d:int, n_head:int):\n",
    "            return [keras.layers.Dense(d) for _ in range(n_head)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "#### InterpretableMultiHeadAttention\n",
       "\n",
       ">      InterpretableMultiHeadAttention (n_head:int, d_model:int,\n",
       ">                                       training:bool=True, dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_head | int |  |  |\n",
       "| d_model | int |  |  |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| dropout | float | 0.0 | Will be ignored if `training=False` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "#### InterpretableMultiHeadAttention\n",
       "\n",
       ">      InterpretableMultiHeadAttention (n_head:int, d_model:int,\n",
       ">                                       training:bool=True, dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_head | int |  |  |\n",
       "| d_model | int |  |  |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| dropout | float | 0.0 | Will be ignored if `training=False` |"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "#| echo: false\n",
    "\n",
    "show_doc(InterpretableMultiHeadAttention, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "imha = InterpretableMultiHeadAttention(n_head=8, d_model=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 17])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "grn.shape # B, T, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 5])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "mask = get_decoder_mask(grn)\n",
    "mask.shape # B, T, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 0.53399706, -0.34997872],\n",
       "        [-1.0885253 ,  1.4489189 ],\n",
       "        [ 1.0975373 , -1.2146299 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5, 5), dtype=float32, numpy=\n",
       " array([[[2.15666671e-03, 9.89674389e-01, 8.07477813e-03, 9.13843178e-05,\n",
       "          2.73770206e-06],\n",
       "         [0.00000000e+00, 6.24240518e-01, 2.27672309e-01, 9.62720364e-02,\n",
       "          5.18151261e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 7.74442792e-01, 1.72618762e-01,\n",
       "          5.29384129e-02],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.69353878e-01,\n",
       "          4.30646122e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]],\n",
       " \n",
       "        [[3.52688372e-01, 2.05759197e-01, 1.31885797e-01, 1.62114590e-01,\n",
       "          1.47552103e-01],\n",
       "         [0.00000000e+00, 1.14922486e-01, 1.40755579e-01, 1.44064263e-01,\n",
       "          6.00257635e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 3.83377731e-01, 2.12374806e-01,\n",
       "          4.04247433e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.58998203e-02,\n",
       "          9.04100239e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]],\n",
       " \n",
       "        [[9.92463589e-01, 1.17329604e-04, 6.22842507e-03, 2.15540611e-04,\n",
       "          9.74971801e-04],\n",
       "         [0.00000000e+00, 8.60778168e-02, 5.10123014e-01, 1.65531754e-01,\n",
       "          2.38267407e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 3.45019311e-01, 3.23296458e-01,\n",
       "          3.31684262e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.29940689e-01,\n",
       "          3.70059252e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "          1.00000000e+00]]], dtype=float32)>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "imha(grn, grn, grn, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalFusionTransformer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Data params\n",
    "        time_steps:int,\n",
    "        input_size:int,\n",
    "        output_size:int,\n",
    "        category_counts:int,\n",
    "        n_workers:int, # Number of multiprocessing workers\n",
    "\n",
    "        # TFT params\n",
    "        input_obs_loc,\n",
    "        static_input_loc,\n",
    "        known_regular_input_idx,\n",
    "        known_categorical_input_idx,\n",
    "        column_definition,\n",
    "\n",
    "        # Network params\n",
    "        quantile:list=[0.1, 0.5, 0.9], # List of quantiles the model should forecast\n",
    "        hidden_layer_size:int=30, # Size of hidden layer\n",
    "        dropout_rate:float=0.0, # Dropout ratio (between 0.0, inclusive, and less than 1.0)\n",
    "        num_encoder_steps:int=4,\n",
    "        num_stacks:int=4,\n",
    "        num_heads:int=4,\n",
    "        \n",
    "        # Training params\n",
    "        max_gradient_norm:float=1.0, # \n",
    "        learning_rate:float=0.001,\n",
    "        minibatch_size:int=64,\n",
    "        num_epochs:int=100,\n",
    "        early_stopping_patience:int=5,\n",
    "        use_gpu:bool=True\n",
    "    ):\n",
    "        self.time_steps = time_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size # Number of periods to be forecasted\n",
    "        self.category_counts = category_counts\n",
    "        self.n_workers = n_workers # Number of multiprocessing workers\n",
    "        \n",
    "        self.input_obs_loc = input_obs_loc\n",
    "        self.static_input_loc = static_input_loc\n",
    "        self.known_regular_input_idx = known_regular_input_idx\n",
    "        self.known_categorical_input_idx = known_categorical_input_idx\n",
    "        self.column_definition = column_definition\n",
    "\n",
    "        self.quantile = quantile # List of quantiles the model should forecast\n",
    "        self.hidden_layer_size = hidden_layer_size # Size of hidden layer\n",
    "        self.dropout_rate = dropout_rate # Dropout ratio (between 0.0, inclusive, and less than 1.0)\n",
    "        self.num_encoder_steps = num_encoder_steps\n",
    "        self.num_stacks = num_stacks\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        self.learning_rate = learning_rate\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def __get_tft_embeddings(\n",
    "        self,\n",
    "        all_inputs # Input tensor of dimensions (batch, time steps, num variables)\n",
    "    ):\n",
    "        # Transform raw inputs to embeddings\n",
    "        # For continuous variables: linear transformation\n",
    "        # For categorical variables: embeddings\n",
    "        \n",
    "        num_categorical_variables = len(self.category_counts)\n",
    "        num_regular_variables = self.input_size - num_categorical_variables\n",
    "\n",
    "        embedding_sizes = [\n",
    "            self.hidden_layer_size\n",
    "            for i, size in enumerate(self.category_counts)\n",
    "        ]\n",
    "\n",
    "        embeddings = [\n",
    "            keras.Sequential([\n",
    "                layers.InputLayer([self.time_steps]),\n",
    "                layers.Embedding(\n",
    "                    self.category_counts[i],\n",
    "                    embedding_sizes[i],\n",
    "                    input_length=self.time_steps,\n",
    "                    dtype='float32'\n",
    "                )\n",
    "            ])\n",
    "            for i in range(num_categorical_variables)\n",
    "        ]\n",
    "\n",
    "        regular_inputs, categorical_inputs = \\\n",
    "            all_inputs[:, :, :num_regular_variables], \\\n",
    "            all_inputs[:, :, num_regular_variables:]\n",
    "\n",
    "        embedded_inputs = [\n",
    "            embeddings[i](categorical_inputs[Ellipsis, i])\n",
    "            for i in range(num_categorical_variables)\n",
    "        ]\n",
    "\n",
    "        # static inputs\n",
    "        if self._static_input_loc:\n",
    "            st_inp_dense = [\n",
    "                layers.Dense(self.hidden_layer_size)(\n",
    "                    regular_inputs[:, 0, i:i + 1]\n",
    "                )\n",
    "                for i in range(num_regular_variables)\n",
    "                if i in self._static_input_loc\n",
    "            ]\n",
    "            st_inp_embed = [\n",
    "                embedded_inputs[i][:, 0, :]\n",
    "                for i in range(num_categorical_variables)\n",
    "                if  i + num_regular_variables in self._static_input_loc\n",
    "            ]\n",
    "            static_inputs = st_inp_dense + st_inp_embed\n",
    "        else:\n",
    "            static_inputs = None\n",
    "\n",
    "        # Targets\n",
    "        past_inputs = keras.ops.stack([\n",
    "            linear_layer(\n",
    "                size=self.hidden_layer_size,\n",
    "                activation=None,\n",
    "                use_time_distributed=True\n",
    "            )(regular_inputs[Ellipsis, i:i + 1])\n",
    "        ], axis=-1)\n",
    "\n",
    "        # past inputs: observed but not known a priori\n",
    "        wired_embeddings = [\n",
    "            embeddings[i](categorical_inputs[:,:,i])\n",
    "            for i in range(num_categorical_variables)\n",
    "            if i not in self._known_categorical_input_idx \\\n",
    "                and i + num_regular_variables not in self._input_obs_loc    \n",
    "        ]\n",
    "        unknown_inputs = [\n",
    "            linear_layer(\n",
    "                size=self.hidden_layer_size,\n",
    "                activation=None,\n",
    "                use_time_distributed=True\n",
    "            )(regular_inputs[Ellipsis, i:i + 1])\n",
    "            for i in range(regular_inputs.shape[-1])\n",
    "            if i not in self._known_categorical_input_idx \\\n",
    "                and i + num_regular_variables not in self._input_obs_loc    \n",
    "        ]\n",
    "        if wired_embeddings + unknown_inputs:\n",
    "            unknown_inputs = keras.ops.stack(wired_embeddings + unknown_inputs, axis=-1)\n",
    "        else:\n",
    "            unkown_inputs = None\n",
    "\n",
    "        # a priori known inputs\n",
    "        known_regular_inputs = [\n",
    "            linear_layer(\n",
    "                size=self.hidden_layer_size,\n",
    "                activation=None,\n",
    "                use_time_distributed=True\n",
    "            )(regular_inputs[Ellipsis, i:i + 1])\n",
    "            for i in self._known_regular_input_idx\n",
    "            if i not in self._static_input_loc\n",
    "        ]\n",
    "        known_categorical_inputs = [\n",
    "            embedded_inputs[i]\n",
    "            for i in self._known_categorical_input_idx\n",
    "            if i + num_regular_variables not in self._static_input_loc\n",
    "        ]\n",
    "        known_combined_layer = keras.ops.stack(\n",
    "            known_regular_inputs + known_categorical_inputs,\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        return unknown_inputs, known_combined_layer, past_inputs, static_inputs\n",
    "\n",
    "    def _build_base_graph(self):\n",
    "        # Build the graph, defining the layers of the TFT\n",
    "        \n",
    "\n",
    "        ### <TFTInputs>\n",
    "        all_inputs = layers.Input(\n",
    "            shape=(self.time_steps, self.input_size) # Argument `shape` does not include batch size\n",
    "        )\n",
    "        unknown_inputs, known_combined_layer, past_inputs, static_inputs \\\n",
    "            = self.__get_tft_embeddings(all_inputs)\n",
    "        ### </TFTInputs>\n",
    "\n",
    "        # first we isolate the known future inputs and observed past inputs\n",
    "        if unknown_inputs is not None:\n",
    "            historical_inputs = keras.ops.concatenate([\n",
    "                unknown_inputs[:, :self.num_encoder_steps, :],\n",
    "                known_combined_layer[:, :self.num_encoder_steps, :],\n",
    "                past_inputs[:, :self.num_encoder_steps, :]\n",
    "            ], axis=1)\n",
    "        else:\n",
    "            historical_inputs = keras.ops.concatenate([\n",
    "                known_combined_layer[:, :self.num_encoder_steps, :],\n",
    "                past_inputs[:, :self.num_encoder_steps, :]\n",
    "            ])\n",
    "        \n",
    "        # and then we isolate the known future inputs\n",
    "        future_inputs = known_combined_layer[:, :self.num_encoder_steps, :]\n",
    "\n",
    "        # static vars\n",
    "        static_encoder, static_weights = static_variable_selection(static_inputs)\n",
    "\n",
    "        # Static covariate encoders\n",
    "        # These integrate static features into the network through encoding of context vectors\n",
    "        # that condition the time-varying dynamics\n",
    "        self.static_context_variable_selection = gated_residual_network( #c_s\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all time steps?\n",
    "        )\n",
    "        self.static_context_enrichment = gated_residual_network( # c_3\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all time steps?\n",
    "        )\n",
    "        self.static_context_state_h = gated_residual_network( # c_h\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all time steps?\n",
    "        )\n",
    "        self.static_context_state_c = gated_residual_network( # c_c\n",
    "            x=static_encoder, # Network inputs\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GRN\n",
    "            output_size=self.hidden_layer_size, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=False, # Apply the GRN across all time steps?\n",
    "        )\n",
    "\n",
    "        historical_features, historical_flags, _ = temporal_variable_selection(\n",
    "            embedding=historical_inputs,\n",
    "            context=self.static_context_variable_selection,\n",
    "            hidden_layer_size=self.hidden_layer_size,\n",
    "            dropout_rate=self.dropout_rate)\n",
    "        future_features, future_flags, _ = temporal_variable_selection(\n",
    "            embedding=future_inputs,\n",
    "            context=self.static_context_variable_selection,\n",
    "            hidden_layer_size=self.hidden_layer_size,\n",
    "            dropout_rate=self.dropout_rate)\n",
    "\n",
    "        # Locality enhancement (Section 4.5.1 in paper) with seq-to-seq layer\n",
    "\n",
    "        # LSTM layers: LSTM Encoder for encoding past inputs\n",
    "        history_lstm, state_h, state_c = layers.LSTM(\n",
    "            units=self.hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            stateful=False,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            recurrent_dropout=0,\n",
    "            unroll=False,\n",
    "            use_bias=True)(\n",
    "                inputs=historical_features,\n",
    "                initial_state=[\n",
    "                    self.static_context_state_h, # short-term state\n",
    "                    self.static_context_state_c  # long-term state\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # LSTM layers: LSTM Decoder for decoding future inputs\n",
    "        future_lstm = layers.LSTM(\n",
    "            units=self.hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=False,\n",
    "            stateful=False,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            recurrent_dropout=0,\n",
    "            unroll=False,\n",
    "            use_bias=True)(\n",
    "                inputs=future_features,\n",
    "                initial_stage=[\n",
    "                    state_h, # short-term state\n",
    "                    state_c  #long-term state\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        lstm_layer = keras.ops.concatenate([history_lstm, future_lstm], axis=1)\n",
    "\n",
    "        # Apply gated skip connection (Gate followed by Add & Norm)\n",
    "\n",
    "        input_embeddings = keras.ops.concatenate(\n",
    "            [historical_features, future_features],\n",
    "            axis=1\n",
    "        )\n",
    "        lstm_layer, _ = apply_gating_layer(\n",
    "            x=lstm_layer, # Input tensors (batch first)\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GLU\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=True, # Apply the GLU across all time steps?\n",
    "            activation=None # Activation function\n",
    "        )\n",
    "        temporal_feature_layer = add_and_norm([lstm_layer, input_embeddings])\n",
    "\n",
    "        # Temporal Fusion Decoder (TFT, Purple box in Fig. 2)\n",
    "        # contains three steps\n",
    "        # TFT 1st step: Static enrichment\n",
    "        #   - enhances the temporal features with static metadata (Eq. 18)\n",
    "        \n",
    "        expanded_static_context_c_e = keras.ops.expand_dims(\n",
    "            self.static_context_enrichment,\n",
    "            axis=1\n",
    "        )\n",
    "        enriched, _ = gated_residual_network( # $\\theta(t, n) = \\text{GRN}_{\\theta}(\\tilde{\\theta}(t, n), c_e)\n",
    "            x=temporal_feature_layer,\n",
    "            hidden_layer_size=self.hidden_layer_size,\n",
    "            output_size=self.hidden_layer_size,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_time_distributed=True,\n",
    "            additional_context=expanded_static_context_c_e,\n",
    "            return_gate=True\n",
    "        )\n",
    "\n",
    "        # TFT 2nd step: Temporal self-attention\n",
    "\n",
    "        self_attention_layer = InterpretableMultiHeadAttention(\n",
    "            n_head=self.num_heads,\n",
    "            d_model=self.hidden_layer_size,\n",
    "            dropout=self.dropout_rate # Will be ignored if `training=False`\n",
    "        )\n",
    "        mask = get_decoder_mask(enriched)\n",
    "        post_attn, self_attention = self_attention_layer( # $B(t) = \\text{IMHA}(\\Theta(t), \\Theta(t), \\Theta(t))$\n",
    "            q=enriched,\n",
    "            k=enriched,\n",
    "            v=enriched,\n",
    "            mask=mask\n",
    "        )\n",
    "        post_attn, _ = apply_gating_layer( #$\\text{GLU}_{\\delta}(\\beta(t, n))$\n",
    "            x=post_attn, # Input tensors (batch first)\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GLU\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            use_time_distributed=True, # Apply the GLU across all time steps?\n",
    "            activation=None # Activation function\n",
    "        )\n",
    "        post_attn = add_and_norm([post_attn, enriched]) # \\delta(t, n) = \\text{LayerNorm}(\\theta(t, n) + $\\text{GLU}_{\\delta}(\\beta(t, n)))$\n",
    "\n",
    "        # TFT 3rd step: Position-wise feed-forward\n",
    "        decoder = gated_residual_network(\n",
    "            x=post_attn,\n",
    "            hidden_layer_size=self.hidden_layer_size,\n",
    "            output_size=self.hidden_layer_size,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            use_time_distributed=True,\n",
    "            additional_context=None,\n",
    "            return_gate=False\n",
    "        )\n",
    "\n",
    "        # final skip connection\n",
    "        decoder, _ = apply_gating_layer(\n",
    "            x=decoder, # Input tensors (batch first)\n",
    "            hidden_layer_size=self.hidden_layer_size, # Dimension of the GLU\n",
    "            dropout_rate=self.dropout_rate, # Dropout rate\n",
    "            activation=None # Activation function\n",
    "        )\n",
    "        transformer_layer = add_and_norm([decoder, temporal_feature_layer])\n",
    "\n",
    "        # the function also returns the attention components\n",
    "        # for explainability analyses\n",
    "        attention_components = {\n",
    "            \"temporal_attention_weights\": self_attention,\n",
    "            \"variable_selection_weights_static_inputs\": static_weights[Ellipsis, 0],\n",
    "            \"variable_selection_weights_past_inputs\": historical_flags[Ellipsis, 0, :],\n",
    "            \"variable_selection_weights_future_inputs\": future_flags[Ellipsis, 0, :]\n",
    "        }\n",
    "\n",
    "        return transformer_layer, all_inputs, attention_components\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build model and define training losses\n",
    "\n",
    "        transformer_layer, all_inputs, self._attention_components = self._build_base_graph()\n",
    "        outputs = keras.layers.TimeDistributed(\n",
    "            keras.layers.Dense(self.output_size * len(self.quantiles))\n",
    "        )(transformer_layer[Ellipsis, self.num_encoder_steps:, :])\n",
    "        model = keras.Model(inputs=all_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the TFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this example, we will use a simple inflation panel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gingado.utils import list_all_dataflows, load_SDMX_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 08:54:00,309 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n",
      "2023-10-31 08:54:19,798 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n",
      "2023-10-31 08:54:39,258 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n",
      "2023-10-31 08:54:40,137 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n",
      "2023-10-31 08:54:43,320 pandasdmx.reader.sdmxml - DEBUG: Truncate sub-microsecond time in <Prepared>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ABS_XML  ABORIGINAL_POP_PROJ                 Projected population, Aboriginal and Torres St...\n",
       "         ABORIGINAL_POP_PROJ_REMOTE          Projected population, Aboriginal and Torres St...\n",
       "         ABS_ABORIGINAL_POPPROJ_INDREGION    Projected population, Aboriginal and Torres St...\n",
       "         ABS_ACLD_LFSTATUS                   Australian Census Longitudinal Dataset (ACLD):...\n",
       "         ABS_ACLD_TENURE                     Australian Census Longitudinal Dataset (ACLD):...\n",
       "                                                                   ...                        \n",
       "UNSD     DF_UNData_UNFCC                                                       SDMX_GHG_UNDATA\n",
       "WB       DF_WITS_Tariff_TRAINS                                WITS - UNCTAD TRAINS Tariff Data\n",
       "         DF_WITS_TradeStats_Development                             WITS TradeStats Devlopment\n",
       "         DF_WITS_TradeStats_Tariff                                      WITS TradeStats Tariff\n",
       "         DF_WITS_TradeStats_Trade                                        WITS TradeStats Trade\n",
       "Name: dataflow, Length: 3137, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflows = list_all_dataflows()\n",
    "dflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer(\n",
    "    time_steps=12,\n",
    "    input_size=20,\n",
    "    output_size=4,\n",
    "    category_counts=5,\n",
    "    n_workers=2, # Number of multiprocessing workers\n",
    "\n",
    "    # TFT params\n",
    "    input_obs_loc=24,\n",
    "    static_input_loc=24,\n",
    "    known_regular_input_idx=24,\n",
    "    known_categorical_input_idx=24,\n",
    "    column_definition=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft.time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# References {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv_gingado')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977c2d9435ad3a481cf1bbece8d5ecb19e078de55648a0a0bad32b79c2e18340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
