{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Experimenting with TFT\"\n",
    "author: Douglas Araujo\n",
    "format: \n",
    "    html:\n",
    "        toc: true\n",
    "        toc-location: right\n",
    "        number-sections: true\n",
    "        code-fold: true\n",
    "        code-tools: true\n",
    "bibliography: ref.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the temporal fusion transformers [@lim2021temporal] architecture, and ports it over to keras 3 while making some punctual improvements.\n",
    "\n",
    "The original repository is: https://github.com/google-research/google-research/tree/master/tft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import keras_core as keras\n",
    "from keras_core import layers\n",
    "from fastcore import docments\n",
    "from nbdev.showdoc import show_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **time distributed**: \n",
    "  * applies same layer to each of the timesteps in the data\n",
    "    * in other words, a layer with the exact same weights\n",
    "  * indices:\n",
    "    * index 0: batch\n",
    "    * index 1: time\n",
    "    * indices 2...: data\n",
    "  * More info: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated residual network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer\n",
    "\n",
    "* dedicated implementation to better control use of time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(size:int, # Output size\n",
    "                 activation:str|callable|None=None, # Activation function\n",
    "                 use_time_distributed:bool=False, # Apply the layer across all timesteps?\n",
    "                 use_bias:bool=True # Include bias in the layer?\n",
    ")->keras.src.layers.core.dense.Dense: # Dense layer\n",
    "    \"Linear layer.\"\n",
    "\n",
    "    linear = keras.layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "    if use_time_distributed:\n",
    "        linear = keras.layers.TimeDistributed(linear)\n",
    "    return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layer\n",
    "\n",
    "* dedicated implementation to better control use of time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(\n",
    "    size:int, # Output size\n",
    "    activation:str|callable|None=None, # Activation function\n",
    "    use_time_distributed:bool=False, # Apply the layer across all timesteps?\n",
    "    use_bias:bool=True # Include bias in the layer?\n",
    ")->keras.src.layers.core.dense.Dense: # Dense layer\n",
    "    \"Dense layer\"\n",
    "\n",
    "    dense = layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "    if use_time_distributed:\n",
    "        dense = layers.TimeDistributed(dense)\n",
    "    return dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "layer_size = 16\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# dense layer\n",
    "dense = dense_layer(size=layer_size, use_time_distributed=True)\n",
    "\n",
    "# output dimensions: batches / timesteps / layer size\n",
    "assert dense(x).shape == [batch_size, n_timesteps, layer_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now showing that the time-distributed layer applies the same weights at all timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "x = np.ones((1, n_timesteps, n_features))\n",
    "timesteps_equal = []\n",
    "for i in range(n_timesteps-1):\n",
    "    timesteps_equal.append((np.array_equal(dense(x)[0,0,:], dense(x)[0,i+1,:])))\n",
    "\n",
    "assert np.all(timesteps_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### dense_layer\n",
       "\n",
       ">      dense_layer (size:int, activation:Union[str,<built-\n",
       ">                   infunctioncallable>,NoneType]=None,\n",
       ">                   use_time_distributed:bool=False, use_bias:bool=True)\n",
       "\n",
       "Dense layer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| size | int |  | Output size |\n",
       "| activation | str \\| callable \\| None | None | Activation function |\n",
       "| use_time_distributed | bool | False | Apply the layer across all timesteps? |\n",
       "| use_bias | bool | True | Include bias in the layer? |\n",
       "| **Returns** | **keras.src.layers.core.dense.Dense** |  | **Dense layer** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### dense_layer\n",
       "\n",
       ">      dense_layer (size:int, activation:Union[str,<built-\n",
       ">                   infunctioncallable>,NoneType]=None,\n",
       ">                   use_time_distributed:bool=False, use_bias:bool=True)\n",
       "\n",
       "Dense layer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| size | int |  | Output size |\n",
       "| activation | str \\| callable \\| None | None | Activation function |\n",
       "| use_time_distributed | bool | False | Apply the layer across all timesteps? |\n",
       "| use_bias | bool | True | Include bias in the layer? |\n",
       "| **Returns** | **keras.src.layers.core.dense.Dense** |  | **Dense layer** |"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "show_doc(dense_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated linear unit (GLU)\n",
    "\n",
    "* Introduced by @dauphin2017language\n",
    "* The GLU is part of the Gated Residual Network (GRN) block\n",
    "* Using input $\\gamma \\in \\mathbb{R}^{d_{\\text{model}}}$ and the subscript $\\omega$ to index weights, $\\text{GLU}_{\\omega}(\\gamma) = \\sigma(W_{4, \\omega} \\gamma + b_{4, \\omega}) \\odot (W_{5, \\omega} \\gamma + b_{5, \\omega})$\n",
    "* As can be seen above, the result could be very close to zero through the Hadamard multipliciation, which in practice means that the network would not be affected by that data (ie, it would be gated out)\n",
    "* *\"GLUs reduce the vanishing gradient problem for deep architectures by providing a linear path for gradients while retaining non-linear capabilities\"*\n",
    "* *\"provide flexibility to suppress any parts of the architecture that are not required for a given dataset\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "\n",
    "def apply_gating_layer(\n",
    "    x, # Input tensors (batch first)\n",
    "    hidden_layer_size:int, # Dimension of the GLU\n",
    "    dropout_rate:float|None=None, # Dropout rate\n",
    "    use_time_distributed:bool=True, # Apply the GLU across all timesteps?\n",
    "    activation:str|callable=None # Activation function\n",
    "): # Tuple of (GLU output tensors, gated_layer)\n",
    "    \"Gated Linear Unit (GLU) layer\"\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    activation_layer = layers.Dense(\n",
    "                hidden_layer_size,\n",
    "                activation=activation\n",
    "            )\n",
    "\n",
    "    gated_layer = layers.Dense(\n",
    "                hidden_layer_size,\n",
    "                activation='sigmoid'\n",
    "            )\n",
    "\n",
    "    if use_time_distributed:\n",
    "        activation_layer = layers.TimeDistributed(activation_layer)(x)\n",
    "        gated_layer = layers.TimeDistributed(gated_layer)(x)\n",
    "    else:\n",
    "        activation_layer = activation_layer(x)\n",
    "        gated_layer = gated_layer(x)\n",
    "\n",
    "    return layers.Multiply()([activation_layer, gated_layer]), gated_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of GLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "hidden_layer_size = 16\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# output dimensions: batches / timesteps / hidden_layer_size\n",
    "assert apply_gating_layer(x=x, hidden_layer_size=hidden_layer_size)[0].shape == [batch_size, n_timesteps, hidden_layer_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### apply_gating_layer\n",
       "\n",
       ">      apply_gating_layer (x, hidden_layer_size:int,\n",
       ">                          dropout_rate:Optional[float]=None,\n",
       ">                          use_time_distributed:bool=True,\n",
       ">                          activation:Union[str,<built-\n",
       ">                          infunctioncallable>]=None)\n",
       "\n",
       "Gated Linear Unit (GLU) layer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Input tensors (batch first) |\n",
       "| hidden_layer_size | int |  | Dimension of the GLU |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GLU across all timesteps? |\n",
       "| activation | str \\| callable | None | Activation function |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### apply_gating_layer\n",
       "\n",
       ">      apply_gating_layer (x, hidden_layer_size:int,\n",
       ">                          dropout_rate:Optional[float]=None,\n",
       ">                          use_time_distributed:bool=True,\n",
       ">                          activation:Union[str,<built-\n",
       ">                          infunctioncallable>]=None)\n",
       "\n",
       "Gated Linear Unit (GLU) layer\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Input tensors (batch first) |\n",
       "| hidden_layer_size | int |  | Dimension of the GLU |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GLU across all timesteps? |\n",
       "| activation | str \\| callable | None | Activation function |"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "show_doc(apply_gating_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip connection\n",
    "\n",
    "Adds inputs to layer, ie \"skip connection\", and then implements layer normalisation [@ba2016layer]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_norm(\n",
    "    x_list # List of input tensors (of the same dimension) for skip connection\n",
    "    ):\n",
    "    \"Adds tensors with same dimensions and then normalises layer\"\n",
    "    tmp = layers.Add()(x_list)\n",
    "    return layers.LayerNormalization()(tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of skip connections + layer normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x1 = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "x2 = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# output dimensions: batches / timesteps / features\n",
    "x1x2 = add_and_norm(x_list=[x1, x2])\n",
    "assert x1.shape == x1x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean values (normalised should be around 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.0878034 ,  0.15579257,  0.02132437, -0.01665834,  0.09834406],\n",
       "        [-0.08403851, -0.09840068, -0.08497837,  0.08315674,  0.04145419],\n",
       "        [ 0.03267195, -0.15613629, -0.06508664, -0.07177194, -0.00536668]]),\n",
       " array([[ 0.08824437,  0.07657852, -0.11315156, -0.14589392, -0.10032865],\n",
       "        [ 0.00629648, -0.23405682,  0.13411307, -0.04240066,  0.02123513],\n",
       "        [-0.17501666, -0.04459926,  0.08562279,  0.02482085, -0.12458415]]),\n",
       " array([[-2.80141830e-08,  9.53674295e-09,  1.78813930e-09,\n",
       "          8.34465030e-09, -5.96046434e-10],\n",
       "        [ 9.53674295e-09,  1.54972071e-08, -2.38418574e-09,\n",
       "         -4.76837148e-09, -5.96046457e-09],\n",
       "        [ 3.03983683e-08,  1.23679635e-08, -1.19209291e-08,\n",
       "         -3.31550822e-08, -9.83476678e-09]], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.mean(axis=-1), x2.mean(axis=-1), x1x2.numpy().mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation (normalised should be around 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.94964034, 1.05664469, 0.92412482, 0.96766552, 1.0508922 ],\n",
       "        [0.99617178, 1.09495702, 1.04056727, 0.90764699, 1.05762491],\n",
       "        [0.99494172, 0.9119524 , 1.03807059, 0.94555555, 1.07363401]]),\n",
       " array([[1.01199589, 1.0563091 , 1.07754409, 1.01300685, 0.9101479 ],\n",
       "        [1.09136647, 1.02580908, 0.99869893, 0.98858294, 1.0134959 ],\n",
       "        [0.99310298, 1.01879404, 1.01631997, 1.12171694, 1.09240828]]),\n",
       " array([[0.9997355 , 0.99979895, 0.9997791 , 0.9997775 , 0.99976087],\n",
       "        [0.9997847 , 0.99976426, 0.99978065, 0.99963933, 0.9997341 ],\n",
       "        [0.99973655, 0.9997171 , 0.9997928 , 0.9997747 , 0.9998052 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.std(axis=-1), x2.std(axis=-1), x1x2.numpy().std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### add_and_norm\n",
       "\n",
       ">      add_and_norm (x_list)\n",
       "\n",
       "Adds tensors with same dimensions and then normalises layer\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| x_list | List of input tensors (of the same dimension) for skip connection |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### add_and_norm\n",
       "\n",
       ">      add_and_norm (x_list)\n",
       "\n",
       "Adds tensors with same dimensions and then normalises layer\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| x_list | List of input tensors (of the same dimension) for skip connection |"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "\n",
    "show_doc(add_and_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated residual network (GRN)\n",
    "\n",
    "* The GRN is a key building block of the TFT\n",
    "    * Helps keep information only from relevant input variables\n",
    "    * Also keeps the model as simple as possible by only applying non-linearities when relevant\n",
    "* $\\text{GRN}_{\\omega}(a, c)$:\n",
    "    * *1st step*: $\\eta_{2} = \\text{ELU}(W_{2, \\omega} a + b_{2, \\omega} + W_{3, \\omega} c)$, (where the additional context $c$ might be zero),\n",
    "    * *2nd step*: $\\eta_{1} = W_{1, \\omega} \\eta_{2} + b_{1, w}$,\n",
    "    * *3rd step*: $\\text{LayerNorm}(a + \\text{GLU}_{\\omega}(\\eta_{1}))$\n",
    "* $\\text{ELU}(\\cdot)$ is the Exponential Linear Unit activation function (@clevert2015fast)\n",
    "    * Unlike ReLUs, ELUs allow for negative values, which pushes unit activations closer to zero at a lower computation complexity, and producing more accurate results\n",
    "* $\\text{LayerNorm}(\\cdot)$ is the layer normalisation (@ba2016layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_residual_network(\n",
    "    x, # Network inputs\n",
    "    hidden_layer_size:int, # Dimension of the GRN\n",
    "    output_size:int|None=None, # Size of output layer (if None, same as `hidden_layer_size`)\n",
    "    dropout_rate:float|None=None, # Dropout rate\n",
    "    use_time_distributed:bool=True, # Apply the GLU across all timesteps?\n",
    "    additional_context=None, # Additional context vector to use if relevant\n",
    "    return_gate:bool=False #Whether to return GLU gate for diagnostic purposes\n",
    "):\n",
    "    \"Applies the gated residual network (GRN) as defined in the paper\"\n",
    "    \n",
    "    # Setup skip connection\n",
    "    if output_size is None:\n",
    "        output_size = hidden_layer_size\n",
    "        skip = x\n",
    "    else:\n",
    "        linear = keras.layers.Dense(output_size)\n",
    "        if use_time_distributed:\n",
    "            linear = keras.layers.TimeDistributed(linear)\n",
    "        skip = linear(x)\n",
    "\n",
    "    # 1st step: eta2\n",
    "    hidden = linear_layer(\n",
    "        size=hidden_layer_size, # W2\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=True # b2\n",
    "    )(x)\n",
    "\n",
    "    # \"For instances without a context vector, the GRN simply treates the context input as zero - ie, $c = 0$ in Eq. 4\"\n",
    "    if additional_context is not None: # if c is != 0...\n",
    "        hidden += linear_layer(\n",
    "            size=hidden_layer_size, # W3\n",
    "            activation=None,\n",
    "            use_time_distributed=use_time_distributed,\n",
    "            use_bias=False # no bias for additional context, since there already is bias from the \"main\" calculation of eta2\n",
    "        )(additional_context)\n",
    "\n",
    "    hidden = keras.layers.Activation('elu')(hidden)\n",
    "\n",
    "    # 2nd step: eta1\n",
    "    hidden = linear_layer(\n",
    "        size=hidden_layer_size, # W1\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=True # b1\n",
    "    )(hidden)\n",
    "\n",
    "    # 3rd step: concluding the GRN calculation\n",
    "    gating_layer, gate = apply_gating_layer(\n",
    "        x=hidden,\n",
    "        hidden_layer_size=output_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        activation=None\n",
    "    )\n",
    "\n",
    "    GRN = add_and_norm([skip, gating_layer])\n",
    "\n",
    "    if return_gate:\n",
    "        return GRN, gate\n",
    "    else:\n",
    "        return GRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of GRN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 100\n",
    "hidden_layer_size = 16\n",
    "output_size = 17\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "grn = gated_residual_network(\n",
    "    x=x,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=0,\n",
    "    use_time_distributed=True,\n",
    "    additional_context=None,\n",
    "    return_gate=False\n",
    ")\n",
    "\n",
    "# output dimensions: batches / timesteps / hidden_layer_size\n",
    "assert grn.shape == [batch_size, n_timesteps, output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### gated_residual_network\n",
       "\n",
       ">      gated_residual_network (x, hidden_layer_size:int,\n",
       ">                              output_size:Optional[int]=None,\n",
       ">                              dropout_rate:Optional[float]=None,\n",
       ">                              use_time_distributed:bool=True,\n",
       ">                              additional_context=None, return_gate:bool=False)\n",
       "\n",
       "Applies the gated residual network (GRN) as defined in the paper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Network inputs |\n",
       "| hidden_layer_size | int |  | Dimension of the GRN |\n",
       "| output_size | int \\| None | None | Size of output layer (if None, same as `hidden_layer_size`) |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GLU across all timesteps? |\n",
       "| additional_context | NoneType | None | Additional context vector to use if relevant |\n",
       "| return_gate | bool | False | Whether to return GLU gate for diagnostic purposes |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### gated_residual_network\n",
       "\n",
       ">      gated_residual_network (x, hidden_layer_size:int,\n",
       ">                              output_size:Optional[int]=None,\n",
       ">                              dropout_rate:Optional[float]=None,\n",
       ">                              use_time_distributed:bool=True,\n",
       ">                              additional_context=None, return_gate:bool=False)\n",
       "\n",
       "Applies the gated residual network (GRN) as defined in the paper\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| x |  |  | Network inputs |\n",
       "| hidden_layer_size | int |  | Dimension of the GRN |\n",
       "| output_size | int \\| None | None | Size of output layer (if None, same as `hidden_layer_size`) |\n",
       "| dropout_rate | float \\| None | None | Dropout rate |\n",
       "| use_time_distributed | bool | True | Apply the GLU across all timesteps? |\n",
       "| additional_context | NoneType | None | Additional context vector to use if relevant |\n",
       "| return_gate | bool | False | Whether to return GLU gate for diagnostic purposes |"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: asis\n",
    "\n",
    "show_doc(gated_residual_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention components\n",
    "\n",
    "* Attention mechanisms use relationships between keys $K \\in \\mathbf{R}^{N \\times d_{attention}}$ and queries $Q \\in \\mathbf{R}^{N \\times d_{attention}}$ to scale a vector of values $V \\in \\mathbf{R}^{N \\times d_V}$: $\\text{Attention}(Q, K, V) = A(Q, K) V$\n",
    "    * $N$ is the number of timesteps going into the attention layer (the number of lags plus the number of periods to be forecasted)\n",
    "    * $A(\\cdot)$ is a normalisation function\n",
    "        * After @vaswani2017attention, the canonical choice for $A(\\cdot)$ is the scaled dot-product: $A(Q, K) = \\text{Softmax}(\\frac{Q K^{T}}{\\sqrt{d_{attention}}} )$\n",
    "    \n",
    "* The TFT uses a modified attention head to enhance the explainability of the model\n",
    "* Specifically, the transformer block (multi-head attention) is modified to:\n",
    "    * share values in each head, and\n",
    "    * employ additive aggregation of all heads\n",
    "* More formally, compare the interpretable multi-head attention (used in this paper) with the canonical multi-head attention:\n",
    "    * $\\text{InterpretableMultiHead}(Q, K, V) = \\tilde{H} W_{H}$, with:\n",
    "        * $\\begin{aligned}\\tilde{H} &= \\tilde{A}(Q, K) V W_V \\\\\n",
    "        &= \\{\\frac{1}{m_H} \\sum^{m_{H}}_{h=1} A(Q W^{(h)}_Q, K W^{(h)}_K) \\} V W_V \\\\\n",
    "        &= \\frac{1}{m_H} \\sum^{m_{H}}_{h=1} \\text{Attention}(Q W^{(h)}_Q, K W^{(h)}_K, V W_V)\n",
    "        \\end{aligned}$\n",
    "    * $\\text{MultiHead}(Q, K, V) = [H_1, \\dots, H_{m_H}] W_H$, with:\n",
    "        * $H_h = \\text{Attention}(Q W^{(h)}_Q, K W^{(h)}_K, V W_V^{(h)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder mask for self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_mask(\n",
    "    self_attention_inputs # Inputs to the self-attention layer\n",
    "):\n",
    "    \"Determines shape of decoder mask\"\n",
    "    len_s = keras.ops.shape(self_attention_inputs)[1] # length of inputs\n",
    "    bs = keras.ops.shape(self_attention_inputs)[0] # batch shape\n",
    "    mask = keras.ops.cumsum(keras.ops.eye(len_s), 1) #keras.backend.cumsum(np.eye(len_s, bs))\n",
    "\n",
    "    ### warning: I had to manually implement some batch-wise shape here \n",
    "    ### because the new keras `eye` function does not have a batch_size arg.\n",
    "    ### inspired by: https://github.com/tensorflow/tensorflow/blob/v2.14.0/tensorflow/python/ops/linalg_ops_impl.py#L30\n",
    "    ### <hack>\n",
    "    mask = keras.ops.expand_dims(mask, axis=0)    \n",
    "    mask = keras.ops.tile(mask, (bs, 1, 1))\n",
    "    ### </hack>\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of the decoder mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = get_decoder_mask(grn)\n",
    "\n",
    "assert dec.shape == (batch_size, n_timesteps, n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it produces an upper-triangular matrix of ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_decoder_mask\n",
       "\n",
       ">      get_decoder_mask (self_attention_inputs)\n",
       "\n",
       "Determines shape of decoder mask\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| self_attention_inputs | Inputs to the self-attention layer |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_decoder_mask\n",
       "\n",
       ">      get_decoder_mask (self_attention_inputs)\n",
       "\n",
       "Determines shape of decoder mask\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| self_attention_inputs | Inputs to the self-attention layer |"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_decoder_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention layer\n",
    "\n",
    "* This is the same as Eq. (1) of @vaswani2017attention \n",
    "    * except that in this case the dimension of the value vector is the same $d_{\\text{model}}$ as for the query and key vectors\n",
    "* As discussed in the paper, additive attention outperforms dot product attention for larger $d_{\\text{model}}$ values, so the attention is scaled back to smaller values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention():\n",
    "    def __init__(\n",
    "        self,\n",
    "        training:bool=True, # Whether the layer is being trained or used in inference\n",
    "        attention_dropout:float=0.0 # Will be ignored if `training=False`\n",
    "    ):\n",
    "        self.training = training\n",
    "        self.dropout = keras.layers.Dropout(rate=attention_dropout)\n",
    "        self.activation = keras.layers.Activation('softmax')\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q, # Queries, tensor of shape (?, time, D_model)\n",
    "        k, # Keys, tensor of shape (?, time, D_model)\n",
    "        v, # Values, tensor of shape (?, time, D_model)\n",
    "        mask # Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)\n",
    "    ):\n",
    "        # returns Tuple (layer outputs, attention weights)\n",
    "        scale = keras.ops.sqrt(keras.ops.cast(keras.ops.shape(k)[-1], dtype='float32'))\n",
    "        attention = keras.ops.einsum(\"btd,btd->bt\", q, k) / scale\n",
    "        if mask is not None:\n",
    "            mmask = keras.layers.Lambda(lambda x: (-1e9) * (1. - keras.ops.cast(x, 'float32')))(mask)\n",
    "            attention = keras.layers.Add()([attention, mmask])\n",
    "        attention = self.activation(attention)\n",
    "        if self.training:\n",
    "            attention = self.dropout(attention)\n",
    "        output = keras.ops.einsum(\"bt,btd->bt\", attention, v)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how the `ScaledDotProductAttention` layer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention 1 shape:  (3, 5)\n",
      "attention 2 shape:  (3, 5)\n",
      "attention 3 shape:  (3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[-0.06796159, -0.14928389,  0.35569158, -0.46120387,  0.30438477],\n",
       "        [ 0.29471743, -2.9135132 ,  0.03538444,  0.09945958, -0.01185272],\n",
       "        [ 0.01331887,  0.29213437, -0.04500379,  0.00928352,  0.3467226 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[0.14248843, 0.06488948, 0.14144121, 0.5226409 , 0.12854004],\n",
       "        [0.17114735, 0.7915294 , 0.01030869, 0.02321733, 0.00379721],\n",
       "        [0.34609836, 0.46975043, 0.1107455 , 0.00888185, 0.0645239 ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "batch_size = 3\n",
    "n_timesteps = 5\n",
    "n_features = 13\n",
    "\n",
    "# input dimensions: batches / timesteps / features\n",
    "x_btf = np.random.randn(batch_size*n_timesteps*n_features).reshape([batch_size, n_timesteps, n_features]) \n",
    "\n",
    "# using the same vector for q, k and v just to simplify\n",
    "q=keras.ops.cast(x_btf, 'float32')\n",
    "k=keras.ops.cast(x_btf, 'float32')\n",
    "v=keras.ops.cast(x_btf, 'float32')\n",
    "\n",
    "output, attention = ScaledDotProductAttention()(q=q, k=k, v=v, mask=None)\n",
    "output, attention # both have shape (batch_size, n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (training:bool=True,\n",
       ">                                 attention_dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| attention_dropout | float | 0.0 | Will be ignored if `training=False` |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ScaledDotProductAttention\n",
       "\n",
       ">      ScaledDotProductAttention (training:bool=True,\n",
       ">                                 attention_dropout:float=0.0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| training | bool | True | Whether the layer is being trained or used in inference |\n",
       "| attention_dropout | float | 0.0 | Will be ignored if `training=False` |"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ScaledDotProductAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[0.14248843, 0.06488948, 0.14144121, 0.5226409 , 0.12854004],\n",
       "        [0.17114735, 0.7915294 , 0.01030869, 0.02321733, 0.00379721],\n",
       "        [0.34609836, 0.46975043, 0.1107455 , 0.00888185, 0.0645239 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       " array([[0.18611768, 0.17222127, 0.18592288, 0.2721985 , 0.18353964],\n",
       "        [0.18452014, 0.34314075, 0.15710588, 0.15914705, 0.15608622],\n",
       "        [0.22777675, 0.25775722, 0.18001015, 0.16257665, 0.17187914]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activ = keras.layers.Activation('softmax')\n",
    "attention, activ(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "A small detour to illustrate the softmax function. \n",
    "\n",
    "The $i^{\\text{th}}$ element of $\\text{Softmax}(x)$, with $x \\in \\mathbf{R}^K$ is:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^K e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, see the values below for an input vector $x$ ($K=5$ in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [-inf  -1.   0.   1.   3.]\n",
      "exp(x) =  [ 0.          0.36787944  1.          2.71828183 20.08553692]\n",
      "denominator (sum of exp(x_j), j=1,...,K) =  24.171698192818155\n",
      "softmax(x) =  [0.         0.01521943 0.0413707  0.11245721 0.83095266]\n",
      "sum of softmax(x)_j, j=1,...,K =  1.0\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: show\n",
    "\n",
    "x = np.array([-np.Inf, -1., 0., 1., 3.])\n",
    "keras.layers.Activation('softmax')(x)\n",
    "print(\"x = \", x)\n",
    "print(\"exp(x) = \", np.exp(x))\n",
    "print(\"denominator (sum of exp(x_j), j=1,...,K) = \", sum(np.exp(x)))\n",
    "print(\"softmax(x) = \", np.exp(x) / sum(np.exp(x)))\n",
    "print(\"sum of softmax(x)_j, j=1,...,K = \", sum(np.exp(x) / sum(np.exp(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the softmax function really makes the largest numbers stand out from the rest.\n",
    "\n",
    "Note also that $-\\infty$ results in 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Multi-head attention\n",
    "\n",
    "* When values are shared in each head and then are aggregated additively, each head still lcan learn different temporal patterns (from their own unique queries and keys), but with the same input values.\n",
    "    * In other words, they can be interpreted as an ensemble over the attention weights\n",
    "    * the paper doesn't mention this explicitly, but the ensemble is equally-weighted - maybe there is some performance to be gained by having some way to weight the different attention heads 🤔, such as having a linear layer combining them... will explore in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.InterpretableMultiHeadAttention at 0x23463a89ca0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InterpretableMultiHeadAttention():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head:int,\n",
    "        d_model:int,\n",
    "        training:bool=True, # Whether the layer is being trained or used in inference\n",
    "        dropout:float=0.0 # Will be ignored if `training=False`\n",
    "    ):\n",
    "        self.n_head = n_head\n",
    "        self.d_k = self.d_v = d_k = d_v = d_model # // n_head - the original model divides by number of heads\n",
    "        self.training = training\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # using the same value layer facilitates interpretability\n",
    "        vs_layer = keras.layers.Dense(d_v, use_bias=False)\n",
    "\n",
    "        # creates list of queries, keys and values across heads\n",
    "        self.qs_layers = self._build_layers(d_k, n_head)\n",
    "        self.ks_layers = self._build_layers(d_k, n_head)\n",
    "        self.vs_layers = [vs_layer for _ in range(n_head)]\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.w_o = keras.layers.Dense(d_v, use_bias=False) # W_v in Eqs. (14)-(16), output weight matrix to project internal state to the original TFT\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q, # Queries, tensor of shape (?, time, D_model)\n",
    "        k, # Keys, tensor of shape (?, time, D_model)\n",
    "        v, # Values, tensor of shape (?, time, D_model)\n",
    "        mask=None # Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)\n",
    "    ):\n",
    "        heads = []\n",
    "        attns = []\n",
    "        for i in range(self.n_head):\n",
    "            qs = self.qs_layers[i](q)\n",
    "            ks = self.ks_layers[i](q)\n",
    "            vs = self.vs_layers[i](v)\n",
    "           \n",
    "            head, attn = self.attention(qs, ks, vs, mask)\n",
    "            if self.training:\n",
    "                head = keras.layers.Dropout(self.dropout)(head)\n",
    "            heads.append(head)\n",
    "            attns.append(attn)\n",
    "\n",
    "        outputs = keras.ops.mean(head, axis=0) if self.n_head > 1 else head # H_tilde\n",
    "        outputs = self.w_o(outputs)\n",
    "        if self.training:\n",
    "            outputs = keras.layers.Dropout(self.dropout)(outputs)\n",
    "\n",
    "        return outputs, attn\n",
    "\n",
    "    def _build_layers(self, d:int, n_head:int):\n",
    "            return [keras.layers.Dense(d) for _ in range(n_head)]\n",
    "\n",
    "InterpretableMultiHeadAttention(n_head=8, d_model=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "imha = InterpretableMultiHeadAttention(n_head=8, d_model=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 17])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grn.shape # B, T, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 5, 5])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_decoder_mask(grn)\n",
    "mask.shape # B, T, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 16) (3, 5, 16) (3, 5, 16)\n",
      "attention 1 shape:  (3, 5)\n",
      "attention 2 shape:  (3, 5, 5)\n",
      "attention 3 shape:  (3, 5, 5)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Einsum_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected input 0 to have rank 2 but got: 3 [Op:Einsum] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\do003097\\Coding\\Nowcasting\\TFT\\temporal_fusion_transformers.ipynb Cell 58\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m imha(grn, grn, grn, mask)\n",
      "\u001b[1;32mc:\\Users\\do003097\\Coding\\Nowcasting\\TFT\\temporal_fusion_transformers.ipynb Cell 58\u001b[0m in \u001b[0;36mInterpretableMultiHeadAttention.__call__\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m vs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvs_layers[i](v)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(qs\u001b[39m.\u001b[39mshape, ks\u001b[39m.\u001b[39mshape, vs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m head, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(qs, ks, vs, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     head \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout)(head)\n",
      "\u001b[1;32mc:\\Users\\do003097\\Coding\\Nowcasting\\TFT\\temporal_fusion_transformers.ipynb Cell 58\u001b[0m in \u001b[0;36mScaledDotProductAttention.__call__\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mattention 3 shape: \u001b[39m\u001b[39m\"\u001b[39m, attention\u001b[39m.\u001b[39mshape)            \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m output \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mbt,btd->bt\u001b[39;49m\u001b[39m\"\u001b[39;49m, attention, v)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/do003097/Coding/Nowcasting/TFT/temporal_fusion_transformers.ipynb#Y134sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output, attention\n",
      "File \u001b[1;32mc:\\Users\\do003097\\Envs\\gingado\\lib\\site-packages\\keras_core\\src\\ops\\numpy.py:2303\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(subscripts, *operands)\u001b[0m\n\u001b[0;32m   2301\u001b[0m \u001b[39mif\u001b[39;00m any_symbolic_tensors(operands):\n\u001b[0;32m   2302\u001b[0m     \u001b[39mreturn\u001b[39;00m Einsum(subscripts)\u001b[39m.\u001b[39msymbolic_call(\u001b[39m*\u001b[39moperands)\n\u001b[1;32m-> 2303\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mnumpy\u001b[39m.\u001b[39;49meinsum(subscripts, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[1;32mc:\\Users\\do003097\\Envs\\gingado\\lib\\site-packages\\keras_core\\src\\backend\\tensorflow\\numpy.py:42\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(subscripts, *operands, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(subscripts, \u001b[39m*\u001b[39moperands, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m tfnp\u001b[39m.\u001b[39meinsum(subscripts, \u001b[39m*\u001b[39moperands, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\do003097\\Envs\\gingado\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_math_ops.py:1381\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(subscripts, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1377\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mInvalid value for argument `optimize`. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1378\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mExpected one of \u001b[39m\u001b[39m{\u001b[39m\u001b[39mTrue, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgreedy\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39moptimal\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m}. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1379\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mReceived: optimize=\u001b[39m\u001b[39m{\u001b[39;00moptimize\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1381\u001b[0m res \u001b[39m=\u001b[39m special_math_ops\u001b[39m.\u001b[39;49meinsum(subscripts, \u001b[39m*\u001b[39;49moperands, optimize\u001b[39m=\u001b[39;49mtf_optimize)\n\u001b[0;32m   1382\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\do003097\\Envs\\gingado\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\do003097\\Envs\\gingado\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Einsum_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected input 0 to have rank 2 but got: 3 [Op:Einsum] name: "
     ]
    }
   ],
   "source": [
    "imha(grn, grn, grn, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# References {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gingado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
